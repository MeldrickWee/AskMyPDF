{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain, ConversationalRetrievalChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 182501 characters in your document\n",
      "Now you have 217 documents\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredPDFLoader(\"../data/pdf_data/AI-Practitioner-Handbook.pdf\")\n",
    "#loader = OnlinePDFLoader(\"https://epoch.aisingapore.org/wp-content/uploads/2023/03/AI-Practitioner-Handbook-20230324.pdf\")\n",
    "\n",
    "data = loader.load()\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AI Practitioner Handbook\\n\\nContributed by Engineers from AI Singapore\\n\\nEdited by Kenny WJ Chua, Ryzal Kamis, Siavash Sakhavi, Anand Natarajan, Kevin Oh, Najib Ninaba, Kim Hock Ng and Laurence Liew\\n\\nMar 24, 2023\\n\\nCONTENTS\\n\\n.\\n\\n.\\n\\n.\\n\\n1 Pre-project Phase 1.1 Overview . . 1.2 How can business challenges be translated into AI problems? . . . . . 1.3 What are some data considerations when framing an AI project? 1.4 What are the considerations for reducing technical debt? . . . . . 1.5 How can an engineer assess a client’s AI readiness? . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . . . .\\n\\n. . . . . . . . . . . .\\n\\n. . .\\n\\n. . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . .\\n\\n. . . . . . . . . . . . . . . . .\\n\\n. . . . . .\\n\\n2 Project Management & Technical Leadership . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='2.1 Overview . . . 2.2 How can I build an effective AI development team? . . . . . 2.3 How can I cultivate a cohesive AI development team? . . . . . 2.4 What kind of engineering principles can I set for my development team? . . . . 2.5 How might we simplify and translate technical jargon? . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. .\\n\\n. . . .\\n\\n.\\n\\n. .\\n\\n.\\n\\n3 Collaborative Development Platforms\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n3.1 Overview . . . . 3.2 What are the key platforms required for collaborative ML development? . . . . 3.3 What are some considerations in setting up a project repository? . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . .\\n\\n. . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='4 Literature Review . 4.1 Overview . 4.2 What are some of the factors to consider during literature review? . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . .\\n\\n. . .\\n\\n. . . . . . . .\\n\\n5 Data Management, Exploration & Processing . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n.\\n\\n.\\n\\n.\\n\\n. . .\\n\\n. . . . .\\n\\nIs there a systematic structure for performing exploratory data analysis? . . . . . . . .', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='. . 5.1 Overview . 5.2 Which data storage options are suitable for the project? . . . . . 5.3 5.4 What are some ways to do EDA for CV tasks? . . . . . 5.5 What are the various data split strategies? . . . . . 5.6 How do we make data splits repeatable? . . . . . 5.7 What are some scenarios of bias, unfairness, data leakage in data splits? . . . . . 5.8 How do I build a basic end-to-end workflow? . . . 5.9 How do I enhance my end-to-end workflow? . . . . . . . . . . 5.10 How can I reduce the risks of data poisoning and data extraction? . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n. . . . .\\n\\n. . . . . . . . . . . . . .\\n\\n. . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . .\\n\\n. . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n6 Modelling\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='6.1 Overview . . . . 6.2 What are some internal and external considerations when selecting evaluation metrics? . . 6.3 How can I maximise model reproducibility? . . . . . 6.4 How do I assess model robustness? . . . . 6.5 How do I select classification metrics? . . . . .\\n\\n. . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . .\\n\\n. . . . . . .\\n\\n. . . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\n. .\\n\\n.\\n\\n.\\n\\n5\\n\\n5\\n\\n5\\n\\n7\\n\\n13\\n\\n17\\n\\n19\\n\\n19\\n\\n19\\n\\n21\\n\\n22\\n\\n24\\n\\n27\\n\\n27\\n\\n27\\n\\n29\\n\\n31\\n\\n31\\n\\n31\\n\\n33\\n\\n33\\n\\n33\\n\\n34\\n\\n38\\n\\n43\\n\\n48\\n\\n49\\n\\n51\\n\\n53\\n\\n54\\n\\n57\\n\\n57\\n\\n57\\n\\n59\\n\\n61\\n\\n63\\n\\ni\\n\\n68\\n\\n71\\n\\n73\\n\\n73\\n\\n76\\n\\n76\\n\\n83\\n\\n83\\n\\n83\\n\\n84\\n\\n87\\n\\n87\\n\\n87\\n\\n91', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='6.6 What are some common CV evaluation metrics? . . . . . 6.7 What are some metrics for Named Entity Recognition? . . . . . 6.8 How can we evaluate time-series classification models? . . . . . . . . . . 6.9 . . . 6.10 References 6.11 How can we provide post-hoc explanations for black-box AI models? . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nTime\\n\\n\\n\\nseries classification .\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . .\\n\\n. . . . . . . . . . . .\\n\\n. . . . . . . . . .\\n\\n. . . . . . . . .\\n\\n. . . . . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n. .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n7 7. Solution Delivery 7.1 Overview . . . 7.2 How can I better understand the client’s deployment requirements? . . 7.3 How can we build a minimum viable configuration for CI/CD automation? . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . .', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . . . . . . . .\\n\\n8 Documentation & Handover\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . 8.1 Overview . 8.2 What are some good practices in documenting system architecture and processes? . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . .\\n\\n. .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n9 Cite AI Singapore’s AI Practitioner Handbook\\n\\nii\\n\\nAI Practitioner Handbook\\n\\nIntroduction\\n\\nThis handbook is an accumulation of AI Singapore’s Innovation and Platforms Engineering team’s experience in delivering more than 40 AI Minimum Viable products (MVP) under the 100E programme over the last 5 years. It is an edited volume containing individual original articles written by our AI Engineers, themed around common topics encountered over a typical AI development lifecycle.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='We envisioned this handbook as a useful guide for new AI Engineers joining AI Singapore and to quickly come up to speed on how we execute AI projects. However, the information contained here would also appeal to any new AI Engineers and Managers deploying their first AI project into production.\\n\\nDelivering production AI models goes beyond building AI models in Jupyter notebooks. It includes preliminary data identification, cleaning and curation, followed by building, training and testing the models and finally deploying the model. Throughout the process, AI and ML Ops play a fundamental role to enable the whole end-to-end process.\\n\\nThere are many excellent books and resources on various AI algorithms, and we will not replicate them here. Instead, we will focus on the best practices and knowledge required to deliver an AI project from end to end.\\n\\nCONTENTS\\n\\n1\\n\\nAI Practitioner Handbook\\n\\nIdentifying AI Ready Projects', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI projects executed by AI Singapore in the 100E programme are real-world problem statements from the industry and go through a rigorous review process before it gets approved and onboarded before any engineering work is done.\\n\\nLeveraging the AI Readiness Index (AIRI) developed by AI Singapore, we are able to quickly identify organizations that are AI Unaware, AI Aware, AI Ready or AI Competent.\\n\\nIn the 100E programme, team’s work focuses on AI Ready companies only1.\\n\\nDelivering AI Ready Projects\\n\\nProjects approved under the 100E programme are delivered in 7 months following an Agile methodology, typically over 10-15 sprints.\\n\\n1 AI Singapore and other Singapore agencies have other programmes which assist companies that are AI Unaware and AI Aware.\\n\\n2\\n\\nCONTENTS\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The project team will consist of AI Singapore’s AI Researchers, Engineers, and AI Apprentices2 (from the AI Appren- ticeship Programme) working on the project full-time for the duration of the project. They are supported by Project Managers and the AI and ML Ops teams to deliver the project on time, and with the right infrastructure and architecture powered by established CI/CD pipelines.\\n\\nOur AI Engineers come from diverse academic disciplines and industry experience. In fact, many of our AI Engineers are hired from AI Apprentice cohorts. This handbook will provide new AI Engineers with a quick guide on our best practices to help them quickly become productive.\\n\\nConclusion\\n\\nSince 2017, we have engaged more than 600 companies, approved more than 1003 projects and completed more than 60 of these AI projects under the 100E programme, and trained more than 250 AI Apprentices.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='2 Who are the AI Apprentices? See How Did AI Singapore Build a “200” Strong All-Singaporean AI Engineering Team With the Blue Ocean\\n\\nStrategy?\\n\\n3 Information to date\\n\\nCONTENTS\\n\\n3\\n\\nAI Practitioner Handbook\\n\\nThe Handbook is a live project and will continue to evolve and be updated as we do more projects and learn more.\\n\\nLet us get started on your first AI project!\\n\\n–\\n\\nLaurence Liew, Director (AI Innovation)\\n\\nWhat Our Reviewers Say\\n\\n“Whether your role in an AI project is that of a technical lead, AI model implementor, data manager, domain or business function expert, or business-side project manager, this handbook will accelerate your learning curve for understanding the end-to-end aspects of the AI project.”- Steven Miller, Professor Emeritus of Information Systems, Singapore Management University and co-author of Working with AI, MIT Press', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='“This is a fantastic book because it focuses on an often overlooked aspect of ML education—the actual problems, people and teams you deploy it for. It’s a great resource for anyone who wants to successfully put the theory of ML into practice. BAM!”- Josh Starmer, Founder and CEO at StatQuest\\n\\n“AISG’s release of the AI Practitioner Handbook as a practical and credible guide to accelerate the learning curve of incoming AI scientists and engineers is a generous service to Singapore’s growing AI community.”- Jason Tamara Widjaja, Global AI Lead at a multinational biopharmaceutical company\\n\\n4\\n\\nCONTENTS\\n\\nCHAPTER\\n\\nONE\\n\\nPRE\\n\\n\\n\\nPROJECT PHASE\\n\\n1.1 Overview\\n\\nThis chapter covers sections pertaining to activities that happen before the commencement of development works in a project. Such activities include project scoping, data readiness assessment, and considerations for initial tech stack selection.\\n\\n1.2 How can business challenges be translated into AI problems?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Contributor: Tan Kwan Chet, Lead AI Technical Consultant\\n\\n1.2.1 Framing Problem Statement from the Business Challenge\\n\\nMost AI projects begin when a sponsor has 1 or a few business challenges to solve. During early stages of an AI project, the AI engineer needs to explore the data and methologies that can best tackle the AI problem present in business challenges. Understanding business context and process is a great starting point to frame the AI problem. It allows you to understand the sponsor’s long term goal and identify the problem statement that affects the sponsor’s business or represents a pain point that the sponsor wants to solve.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It takes a lot of creativity to formulate the problem statement. Sponsor tends to speak about their business problems at a high level. So your job is to filter the high level information shared by sponsor to a low level where the problem may relate to their employees who face the pain point on a day-to-day basis. This is important since your AI solution will likely be replacing a repetitive task that their employees may be facing. To form the problem statement, AI Singapore uses concepts from Design Thinking methodology and Design Sprint to help in this could use the following steps (illustrated as a map diagram below):\\n\\n1. Identify the target audience (e.g. employee)\\n\\n2. Define the problem from target audience’s perspective (i.e. understand their pain point)\\n\\n3. Understand when and where the problem is ocurring\\n\\n4. Discover the benefit for the target audience and value for the business\\n\\n5. Convert the information into a HMW (How Might We) problem statement\\n\\n5\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Example\\n\\n\\n\\nMap Diagram\\n\\n1.2.2 Translation of Problem Statement into AI Problem\\n\\nNext, it is wise to decompose the problem statement into different technical requirements of an AI problem. This can be done by asking a series of questions to prompt the sponsor to better describe their technical requirements.\\n\\nExample - Translation from Problem Statement to Technical Requirements\\n\\nSponsor’s Problem Statement: “How might we build an AI model to count cells on microscopy images?”\\n\\n6\\n\\nChapter 1. Pre\\n\\n\\n\\nproject Phase\\n\\nQuestion\\n\\nWhat is the main\\n\\ntask that you are\\n\\nkeen to replace?\\n\\nWhat is the type\\n\\nof the data in\\n\\n\\n\\nvolved?\\n\\nWhat is the out\\n\\n\\n\\nput required?\\n\\nAI Practitioner Handbook\\n\\nRationale This gives you an idea on what is the main task that AI needs to automate\\n\\nTechnical Requirement AI model to detect cells by size', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='By knowing if the data is image/video, text, tabular or mul- timodal, it will help you narrow down to the appropriate AI domain/models to focus on By knowing specifically how the business needs the outputs returned to them, it will help you select the correct algorithm and design the appropriate annotation format\\n\\nIs the main task\\n\\nachievable by AI\\n\\nmodel alone?\\n\\nYou can check if there is a need to use heuristic method or rule-based methods in conjunction with the AI model in order to reach the required outcome\\n\\nHow will the AI\\n\\ninto\\n\\nmodel fit\\n\\nyour\\n\\nbusiness\\n\\nprocesses?\\n\\nThis helps you to understand how the sponsor will use the AI model, in terms of what applications it will connect to, how often inference will be run, and how much human involvement there will be\\n\\nImages of cells taken under mi- croscope', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Masks drawn around detected cells and size estimated -> Com- puter Vision instance segmenta- tion model Apply pixel:nm size conversion to estimate actual sizes of de- tected objects and group them into classes Description of high level appli- cation architecture diagram and infrastructure requirements\\n\\nWith these, you would have gained a better understanding of the business challenge and how it translates into the AI problem. This will give you a coverage of what the sponsor hopes to achieve from its envisioned AI model.\\n\\n1.2.3 References\\n\\nSprint: How to Solve Big Problems and Test New Ideas in Just Five Days\\n\\nDesign, Together\\n\\nData Science for Business\\n\\n1.3 What are some data considerations when framing an AI project?\\n\\nContributor: Tan Kwan Chet, Lead AI Technical Consultant\\n\\n1.3.1 Key Areas in Data\\n\\nQuote from Andrew Ng', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI systems need both code and data, and “all that progress in algorithms means it’s actually time to spend more time on the data,” Ng said at the recent EmTech Digital conference hosted by MIT Technology Review.\\n\\n1.3. What are some data considerations when framing an AI project?\\n\\n7\\n\\nAI Practitioner Handbook\\n\\nData Centricity\\n\\nWith the rise of Data-Centric AI (DCAI) since 2021, there is a switch in focus from code that builds the model to the data that model is trained on. The shift arises because improving the quality of the data could help model to perform better, and in a fairer and more robust manner, than simply improving the code alone.\\n\\n4 Key Data Questions', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Having understood the business challenge and AI problem at hand, this is where you need to delve into understanding the data the sponsor will be providing. You need to assess if there are risks/uncertainties posed within the data collected and annotated for solving the AI problem. Because a good AI model not only relies on algorithms but also quality data.\\n\\nIn practice, there are many considerations and checks for data quality. In this article, we focus on 4 key questions that are highly relevant at the pre-project phase to ensure that the AI problem is feasible and well-scoped. Note that while this is written with a supervised learning approach in mind, most of the considerations are also applicable to other learning approaches.\\n\\n1. Is the target clearly defined and labels available?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Frequently, sponsor may not be able to articulate the target variable of interest. This is where the translation of the business problem in this chapter becomes important. This target variable must be related to a business outcome (e.g. reducing employee’s repetitive tasks, prioritising patients for treatment based on mortality rate) that the sponsor wants to achieve.\\n\\nBased on the translated AI task, this is where you ascertain whether the sponsor has a clear definition for the class/value/object/entity/text etc that is to be inferred. This determines whether quality labels can be defined.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='If the target is of classification nature, it will be ideal to observe the current label proportion (e.g. binary label: “yes”, “no”) to ascertain if there is presence of label data imbalance. If the label data is highly imbalanced (e.g. 5% positive label and 95% negative label), an alternative would be to reframe the AI problem as an anomaly detection problem. More importantly, you would be keen to confirm if the label data corresponds to the definition of the target and the availability of the label data.\\n\\nIf the target belongs to a regression type, the question is more about ensuring the target variable is truly nu- meric/continuous. If upon checking you find that there is only a limited set of discrete values that the target variable can take on, it is better to reframe it as a classification problem. This is a possible scenario in businesses that rely on domain experts to run product experiments by testing different combinations of inputs with varying levels.\\n\\n8\\n\\nChapter 1. Pre', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='project Phase\\n\\nAI Practitioner Handbook\\n\\n2. Does the data include features that can predict the target?\\n\\nAn ideal way to estimate if the relevant features are present would be to ask the sponsor the following questions based on a priori domain knowledge:\\n\\nIf a human were to perform the task, what information would the person rely on to make the prediction?\\n\\nIs this information available for training the model and during inference?\\n\\nThere should be adequate signal and variance within the relevant features. Signal refers to the presence of sufficient information relevant to predicting the target. Variance refers to having sufficient range and diversity of data.\\n\\nThis can be checked via a small, representative sample dataset from the sponsor.\\n\\nfor CV and NLP problems, using a pretrained model to run inference on sample data is a great way to assess the\\n\\n‘information-ness’. Sometimes, you can also ‘eyeball’ the data, since for images and text, WYSIWYG!', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='for tabular problems, you can create a plot of mutual information/correlation between features and the target vari-\\n\\nable. A simple example is shown below for numeric features.\\n\\n1.3. What are some data considerations when framing an AI project?\\n\\n9\\n\\nAI Practitioner Handbook\\n\\nAt times, you will face a situation in which sponsor claims that their domain knowledge is accurate in determining the predictive features while our exploratory data analysis (EDA) lends limited support for it. You could attempt some If it does not denoising techniques (e.g. outlier removal, averaging/binning) and check if the correlation improves. improve, this might be due to some confounding effect of a third feature that may influence the relationship between target and feature of interest.\\n\\n10\\n\\nChapter 1. Pre\\n\\n\\n\\nproject Phase\\n\\nAI Practitioner Handbook\\n\\n3. Is the data at the correct granularity?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It is critical to ensure that the training data and production data share the same level of granularity (e.g. hourly, daily, weekly, monthly or yearly) as what your sponsor expects in prediction. So if the sponsor expects the prediction on an hourly basis and the sponsor collects the data daily, then it will not be possible to build such an AI model to produce a granular prediction based on aggregated data. It is also worthy to note that if expected prediction is at a higher or similar granularity as the training or production data, then it is possible to build an AI model.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Besides matching granularity between training and production data, understanding the correct granularity allows you to estimate the volume of data available for training. For example, if the goal is to aggregate a sensor’s readings from seconds to hours for prediction, because the per-second readings contain mostly noise, then having “86,400 data points” may sound like a large dataset, but in actual fact, it is insufficient as there are only 24 hourly aggregates for you to work with.\\n\\n4. Is the training data representative of the production data?\\n\\nIt is critical that training data is representative of the production data to ensure the trained model will be fair and robust:\\n\\nAvoid unintended bias. If the model is trained on training data that is dominated by a specific label (i.e. majority\\n\\nlabel), then it is very likely that the model is robust in predicting a majority label while it will unlikely predict a\\n\\nminority label.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Improve robustness and generalisability. The distribution of training data will be what the model will learn from.\\n\\nAssuming that the distribution between training and production data is highly similar, this would mean that the\\n\\ntrained model is able to generalise and make robust predictions in varying contexts that are often present in pro-\\n\\nduction data. It also means the model is less prone to immediately drift the moment it is applied in production.\\n\\nSo how we check for representativeness? To do so, you would first need to discuss and identify a few key attributes that capture the defining characteristics of the data. Here is an example for a salient object detection problem, where ‘size of objects’ and ‘whether there are multiple objects’ are 2 attributes. For problems that involve human subjects like customers and patients, defining characteristics could include key demographics like age, gender and ethnicity.\\n\\n1.3. What are some data considerations when framing an AI project?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='11\\n\\nAI Practitioner Handbook\\n\\nOnce you are able to define these, you could request the sponsor to estimate the proportion of their production data broken down by different attributes. For example, below is a breakdown of the distribution of object size for the salient object detection dataset. The goal is then to ensure that the training data matches this production distribution:\\n\\nUsing these 4 key questions will allow you to have a gauge of the quality of the data for building the model. This is an important starting point for building a model that has good predictive power, and it is also fair and robust.\\n\\n12\\n\\nChapter 1. Pre\\n\\n\\n\\nproject Phase\\n\\nAI Practitioner Handbook\\n\\n1.3.2 References\\n\\nData-Centric AI\\n\\nLanding AI\\n\\nWhy it’s time for ‘data-centric artificial intelligence’\\n\\nIs My Data Any Good? A Pre-ML Checklist\\n\\nWhat to do After Deploying your Model to Production\\n\\n1.4 What are the considerations for reducing technical debt?\\n\\nContributor(s): Dylan Poh Guan Kiong, AI Engineer (GitHub)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='1.4.1 What is technical debt?\\n\\nIn software development, technical debt is the accumulation of continuous expenses. It occurs when software engineers prioritise speed of deployment over all other development factors. Fast builds can cause problems that can be very difficult to resolve in the future.\\n\\nSimilar to financial debt, technical debt that is not repaid can accrue “interest,” which makes it more expensive to make modifications in future. However, it might not always be a bad thing as developers might intentionally cut corners to produce a proof of concept to advance initiatives.\\n\\nMartin Fowler offers a more detailed analyses of technical debt quadrant:\\n\\n1.4. What are the considerations for reducing technical debt?\\n\\n13\\n\\nAI Practitioner Handbook\\n\\nTechnical Debt Quadrant Source\\n\\n1.4.2 Deliberate and prudent technical debt\\n\\nDeliberate vs inadvertent', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI engineers may deliberately choose to cut corners, knowing that it would cost them in the long run. For instance, the developer may choose to build and deploy a model without investing in a framework or pipeline, choosing quick release over maintainability. In other words, they consciously trade short-term gains for future expenses.\\n\\nJunior AI engineers might not fully appreciate the benefit of using version control system and inadvertently decide to develop without the relevant tools. They may later find it impossible to roll back the code when the pipelines are broken.\\n\\n14\\n\\nChapter 1. Pre\\n\\n\\n\\nproject Phase\\n\\nAI Practitioner Handbook\\n\\nPrudent vs reckless\\n\\nSome engineering choices, like rapidly prototyping a large number of models to choose the best algorithm, hence over- looking a maintainable pipeline could be a prudent decision since they are aware that they are incurring debt and consider the pros and drawbacks of using the best algorithm versus paying it off sooner.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='However, knowing sound software development principles but writing spaghetti code because of a tight schedule could result in reckless debt.\\n\\n1.4.3 Technical Debt in Machine Learning\\n\\nThe maintenance/infrastructure expenses associated with machine learning project go well beyond those technical debt associated with standard software projects. Jumping onto the hype train and using machine learning to tackle problems that already have a solution or that can be solved quickly using heuristics may be costly and reckless, but using machine learning to replace repetitious work with labor shortage (such as object detection on defects) will be a deliberate choice with long-term benefits.\\n\\nML Code takes up only a small portion of the whole pipeline, and technical debt can accumulate quickly in other processes. Source\\n\\n1.4.4 The 25 Best Practices\\n\\nTechnical debt has already been the subject of extensive research. A list of best practices as recommended by Matthew McAteer is provided here:', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Use interpretability tools like SHAP values\\n\\nUse explainable model types if possible\\n\\nAlways re-train downstream models\\n\\nSet up access keys, directory permissions, and service-level-agreements.\\n\\nUse a data versioning tool.\\n\\nDrop unused files, extraneous correlated features, and maybe use a causal inference toolkit.\\n\\nUse any of the countless DevOps tools that track data dependencies.\\n\\nCheck independence assumptions behind models (and work closely with security engineers.\\n\\nUse regular code-reviews (and/or use automatic code-sniffing tools).\\n\\nRepackage general-purpose dependencies into specific APIs.\\n\\n1.4. What are the considerations for reducing technical debt?\\n\\n15\\n\\nAI Practitioner Handbook\\n\\nGet rid of pipeline jungles with top-down redesign/reimplementation.\\n\\nSet regular checks and criteria for removing code, or put the code in a directory or on a disk far-removed from the\\n\\nbusiness\\n\\n\\n\\ncritical stuff.\\n\\nStay up-to-date on abstractions that are becoming more solidified with time', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Use packages like Typing and Decimal, and don’t use ‘float32’ for all data objects\\n\\nDo not leave all works-in-progress in the same directory. Clean it up or toss it out.\\n\\nMake sure endpoints are accounted, and use frameworks that have similar abstractions between languages\\n\\nMake it so you can set your file paths, hyperparameters, layer type and layer order, and other settings from one\\n\\nlocation\\n\\nMonitor the models’ real-world performance and decision boundaries constantly\\n\\nMake sure distribution of predicted labels is similar to distribution of observed labels\\n\\nPut limits on real-world decisions that can be made by machine learning systems\\n\\nCheck assumptions behind input data\\n\\nMake sure your data isn’t all noise and no signal by making sure your model is at least capable of overfitting\\n\\nUse reproducibility checklists when releasing research code\\n\\nMake a habit of checking and comparing runtimes for machine learning models', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Set aside regular, non-negotiable time for dealing with technical debt (whatever form it might take)\\n\\n1.4.5 Measuring technical debt\\n\\nBefore delving into specifics, this article includes a list of pertinent questions to consider, allowing the reader to get a general idea of the amount of technical debt present in a system:\\n\\nHow easily can an entirely new algorithmic approach be tested at full scale?\\n\\nWhat is the transitive closure of all data dependencies?\\n\\nHow precisely can the impact of a new change to the system be measured?\\n\\nDoes improving one model or signal degrade others?\\n\\nHow quickly can new members of the team be brought up to speed?\\n\\nAnd here is a paper that goes into greater detail and provides additional information about the rubric used to evaluate technical debt.\\n\\n1.4.6 References\\n\\nMachine Learning: The High-Interest Credit Card of Technical Debt\\n\\nHidden Technical Debt in Machine Learning Systems', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction\\n\\n16\\n\\nChapter 1. Pre\\n\\n\\n\\nproject Phase\\n\\n1.5 How can an engineer assess a client’s AI readiness?\\n\\nContributor: Joy Lin, Senior AI Technical Consultant\\n\\nAI Practitioner Handbook\\n\\n1.5.1 Who is this for?\\n\\nThis article is aimed at AI engineers in an organisation like AI Singapore, who are building a solution for another company (henceforth referred to as the client) to take over and implement. In AI Singapore, the business development/presales team, together with the AI engineer, scope for feasible AI projects. In doing so, they assess whether the client has the capability to take over the final solution, integrate and maintain it, as the goal is to enable companies to build their own AI capabilities in the long run. Assuming that the client’s proposed AI solution has an established business value and is ethical, this article narrows the focus to the technical aspects of AI readiness.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='1.5.2 Why does AI engineer need to assess client’s AI capabilities?\\n\\nWhen participating in pre-project scoping, it is beneficial for you to gauge if the client is capable of taking over your solution for deployment. Most AI projects end up in failures during deployment phase due to three main reasons:\\n\\n1. Technical hurdles in implementing/integrating model into existing operations\\n\\n2. Decision makers unwilling to approve change to existing operations\\n\\n3. Model performance not considered strong enough by decision makers\\n\\n1.5.3 What can the AI engineer look out for?\\n\\nTo ensure a successful project, let us focus on three broad areas to help you assess the client’s AI readiness level.\\n\\n1. Organisational readiness\\n\\nIs there an existing technical team who is able to integrate and maintain the AI models? If not, what are the client’s plans to hire the necessary resources?\\n\\nIs the client’s management supportive of AI projects and team expansion if necessary?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Do the client’s management and key stakeholders allow room for experimentation and development?\\n\\nAs an organisation looking to use AI solutions, the client’s management should understand that model building and main- tenance requires iterative experimentation and continuous re-training, in turn supporting the investment of right resources.\\n\\n2. Infrastructure readiness\\n\\nIs there appropriate infrastructure to hold data in a centralised and standardised manner (e.g. data ware- house), or is the client relying on disparate file systems?\\n\\nAre there sufficient computational resources (e.g. CPUs, GPUs, memory) to support model deployment and maintenance for this project? If not, are there plans to acquire more or re-allocate resources?\\n\\nPreparing for the above strengthens the client’s infrastructure capabilities, easing their transition from deployment to integration with minimal disruption to their existing activities.\\n\\n1.5. How can an engineer assess a client’s AI readiness?\\n\\n17', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI Practitioner Handbook\\n\\n3. Data readiness\\n\\nDoes the centralised data warehouse provide consistent data format, up-to-date metadata and a single source of truth?\\n\\nIs there accurate and complete data to be used in this project?\\n\\nIt is important for the client to maintain high quality and consistent data that can be used for re-training when model drifts over time.\\n\\nNote: Click here to read about data readiness on a project level.\\n\\nMore details available below to help in your assessment:\\n\\n1. AI Readiness Index (AIRI) developed by AI Singapore and increasingly used by organisations in Singapore\\n\\n2. Oxford’s Government AI readiness index\\n\\n18\\n\\nChapter 1. Pre\\n\\n\\n\\nproject Phase\\n\\nCHAPTER\\n\\nTWO\\n\\nPROJECT MANAGEMENT & TECHNICAL LEADERSHIP\\n\\n2.1 Overview', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This chapter includes sections on both project management and technical leadership. Project management matters include insights on project lifecycle, managing interactions end-users of the ML solutions, etc. Technical leadership matters relate to managing a team of AI developers, as well as typical activities performed by a technical lead over the lifecycle of a project.\\n\\n2.2 How can I build an effective AI development team?\\n\\nContributor: Kenny WJ Chua, Senior AI Engineer\\n\\n2.2.1 Introduction\\n\\nThe responsbilities of a technical lead are different from those of an individual contributor. Engineers, especially those who are who are unfamiliar with leadership responsibilities, may face a learning curve in fulfilling these responsibilities.\\n\\nA technical lead indirectly delivers an AI project on time and on target by building an effective AI development team. This article provides suggestions on promoting effective teams by contextualising general leadership principles.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In AISG, an engineer serves as technical lead for a small team of apprentices (i.e., ‘developers’). Therefore, the contents of this article would be useful to any AI practitioner who is in a similar leadership role.\\n\\nThis article requires some basic knowledge of a AI project life cycle and typical activities that occur in each stage. The article on building a cohesive AI development team may also be useful for engineers in the role of technical lead.\\n\\n2.2.2 Model the way\\n\\nA technical lead may have a specific mental picture of the deliverables and ideal approach for a given task, but the end result may differ when delegated to developers. One possible reason is imperfect communication, such that developers do not entirely understand the specifications that have been laid out (hence the need for architecture diagrams). Alternatively, less experienced developers may not understand the need to adhere strictly to certain best practices.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In such a context, it is important for the technical lead to be able to demonstrate some of these values and practices that he/she espouses. For example, a technical lead can cultivate a team habit of writing well-structured and documented code by taking on a few of such tasks himself/herself. This may be particularly fruitful when onboarding a new team of junior developers who are not yet familiar with the required standards.\\n\\n19\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='A technical lead can cultivate an environment for innovation by enforcing a no-blame culture for failed experiments. This involves focusing on and rectifying the work process that led to the failure, rather than paying attention to the personnel involved. There is also a need to choose the correct measurement of success. For example, developers should not be judged in terms of model evaluation metrics, which are strongly dependent on data quality which is largely beyond the team’s control. It would perhaps be fairer and more fruitful to assess the developers’ adherence to best practices (e.g., judicious data splits, systematic model experimentation).\\n\\nAs the technical lead, take responsibility for the team’s failures, while giving full credit to the team for success. While there is a need to balance practical considerations of the project, the aim of these activities is to build a safe space that encourages developers to experiment.\\n\\n2.2.3 Challenge the process', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Developers within the team can come from different backgrounds, and can therefore contribute diverse, innovative ideas. Taken discerningly, this can present a significant advantage for the project.\\n\\nComplementing an innovative culture, an effective team also strives for iterative improvement that brings the project closer to established best practices. This requires the technical lead to keep up to date by spending a significant portion of his/her time reading broadly about and evaluating best practices for each stage of the AI project lifecycle. Such information can be obtained by discerningly following tech blogs and influential thought leaders on social media platforms. Reading/research need not be confined to the specific subdomain of the current project, as ideas in other areas may be transferrable or can provide inspiration.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Iterative improvements come at the inter- and intra-project levels. At the inter-project level, an experienced technical lead can evaluate past projects to identify areas of workflow improvement. The technical lead can also network and share regularly with other technical leads in order to facilitate inter-project transfer of ideas. At an intra-project level, Agile philosophy stresses the importance of iterative development. When time allows, the project team can revisit previously written code in order to further improve it to more closely align with best practices.\\n\\n2.2.4 Enable others to act', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The primary responsiblity of the technical lead is to be an enabler rather than a “super contributor”. While a technical lead can contribute directly to the codebase at times, he/she needs to be aware that this comes with a trade-off of spending less time on enablement activities. Enabling activities are those that amplify/multiply the effectiveness of the team. These include researching and evaluating potential approaches, planning and delegation, as well as removing technical blockers.\\n\\nTechnical enablements include boilerplate code and abstract classes, which can help developers onboard more quickly and with greater consistency. Additionally, a robust MLOps workflow provides automation that enables to developers to concentrate of feature development and modelling activities rather than infrastructural work.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Pay particular attention during stand-up meetings and sprint retrospectives. These are platforms during which devel- opers are likely to raise their concerns regarding technical blockers. The technical lead can triage the list in order to identify and resolve the most urgent and important blockers. Relatedly, regular one-on-one meetings with developers can also help encounter personal/interpersonal blockers that may not surface during meetings with the whole team.\\n\\n20\\n\\nChapter 2. Project Management & Technical Leadership\\n\\nAI Practitioner Handbook\\n\\n2.2.5 References\\n\\nThe Leadership Challenge: How to Make Extraordinary Things Happen in Organizations\\n\\nAI project life cycle\\n\\n1-on-1s\\n\\nYour team needs a tech lead, not a lead techie\\n\\n2.3 How can I cultivate a cohesive AI development team?\\n\\nContributor: Kenny WJ Chua, Senior AI Engineer\\n\\n2.3.1 Introduction', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Engineers in charge of a development team take on the role of a technical lead. This entails a set of responsibilities that are distinct from those of an individual contributor. Engineers, especially those who are inexperienced with leadership responsibilities, may face challenges in rallying their team.\\n\\nA cohesive team is one that is well-aligned towards a single, common goal. This article provides suggestions on promoting cohesive teams by operationalising general leadership principles within the context of an AI development team.\\n\\nIn the context of AISG, an engineer plays the role of technical lead for a small team of apprentices (henceforth referred to as ‘developers’). Hence, the contents of this article would be relevant to any AI practitioner who is in a similar role as team lead.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The article assumes that the reader has some (but not necessarily extensive) knowledge of a AI project life cycle and typical activities that occur in each stage. You may also wish to consult the complementary article on building an effective AI development team.\\n\\n2.3.2 Importance of a cohesive team\\n\\nDuring early stages of a project, developers can come onboard with divergent expectations of project deliverables. Even when project deliverables have been specified in writing, there can be varying interpretations regarding the scope of said deliverables and the approaches to achieving them. Developers will also come with their own set of specific expectations on what they wish to learn from a project, e.g., specific tech stacks.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Divergent expectations present a risk to the cohesiveness of the development team. This can make it challenging to come to a consensus on technical decisions. Correspondingly, the risk of miscommunication is also increased, which can lead to time-consuming rework of project components. It is therefore in the team’s best interest to mitigate such risks by promoting alignment on project requirements and individual expectations.\\n\\n2.3. How can I cultivate a cohesive AI development team?\\n\\n21\\n\\nAI Practitioner Handbook\\n\\n2.3.3 Promoting a shared vision\\n\\nThe technical lead can do so by promoting a shared vision for the team. This vision includes the technical direction, scope and expected quality of work. The early stages of the project represents a critical window to align expectations on the project vision, so as to minimise the accumulation of downstream technical debt.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='As much as possible, the project vision ought to be practically and tangibly laid out in the form of system architecture diagrams, rather than left as potentially ambiguous textual/verbal descriptions. Areas that are out-of-scope also need to be made explicit as much as possible, such that the team can come to a common understanding of the work that needs to be prioritised for project delivery.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='An effective vision requires buy-in from the project team. One way to facilitate this is to articulate how developers can benefit from the project. This frequently comes in the form of opportunities for professional/technical growth, e.g., by learning new technologies. However, it should also be carefully hedged that the choice of technologies used in the project need to be guided by project requirements rather than learning needs. While it is critical to foster technical growth amongst junior developers, their learning should occur in the form of on-job-training while keeping project needs as a priority.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In-person or synchronous remote code review sessions during the initial phase of a project present a practical opportunity to align expectations regarding code quality. These initial sessions can be conducted with the entire development team, such that any clarifications regarding coding and documetation styles can be immediately clarified. These initial code reviews will then set the tone for the expected code quality for the rest fo the project.\\n\\n2.3.4 Encourage the heart\\n\\nMorale is a key driver of team success. Maintaining high morale within the team can provide developers with encour- agement and motivation to perform at their best. However, developing under tight deadlines and high expectations is stressful, so it takes a deliberate effort to keep the morale up.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='One way to do so is by regularly celebrating small project milestones with the team. Beyond providing words of affirmation, demonstrate your appreciation in tangible ways that will actually help/be appreciated by the developers. For example, encourage the team to slow down after a particularly challenging sprint, or gather the team for a nice lunch after a milestone deployment. While some of these activities can incur some short-term costs in terms of time, the idea is to provide some time and space for developers to recharge so that can be greater long-term gains.\\n\\n2.3.5 References\\n\\nThe Leadership Challenge: How to Make Extraordinary Things Happen in Organizations\\n\\nAI project life cycle\\n\\n2.4 What kind of engineering principles can I set for my development\\n\\nteam?\\n\\nContributor(s): Siavash Sakhavi, Senior AI Engineer, Co-written with https://rytr.me/', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Principles and values are two important concepts that should be set for any software team. All teams should set certain principles and values for themselves in order to be productive, successful, and happy. These two concepts are what will help guide your projects and keep everyone on the same page.\\n\\n22\\n\\nChapter 2. Project Management & Technical Leadership\\n\\nAI Practitioner Handbook\\n\\n2.4.1 What are the differences between principles and values?\\n\\nThe first step in setting the values of a team is to identify the principles that will guide their behavior. These principles are the underlying beliefs that drive their behavior and decisions.\\n\\nA principle is something that is true for all members of a team, no matter what. These principles are typically agreed upon by the team members at the start of their project or business.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Values on the other hand are more personal to each individual and reflect what they believe in. In order to set your values, you need to first identify your principles and then list what those principles mean for you and how they will affect your actions as an individual on the team.\\n\\n2.4.2 Examples of Engineering Principles\\n\\nThe following are some engineering values that you can consider for your team:\\n\\nLearn with curiosity: Learning is often about following the rules, but this can limit your mind. When you are\\n\\ncurious, you have a spirit of exploration that can lead to creative solutions. Curiosity leads to questions, which lead\\n\\nto experiments and discovery.\\n\\nStrive for simplicity: This includes the use of simple software design to ensure the user is able to understand it\\n\\nwith ease. The engineer should also create documentation that is clear and concise. Simplicity is key in developing\\n\\nsoftware that has a low bug rate as well as being easy to update.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Articulate your mental model: A software engineer’s mental model is their internal representation of the system\\n\\nbeing developed. It is the most important part of their job. They should be able to articulate this mental model to\\n\\nother developers in order for them to be able to understand what needs to be done, as well as what has been done\\n\\nand why it was done that way.\\n\\nTeach with compassion: It is essential that software engineers are willing to teach their team members who may\\n\\nnot know as much about the given task. Not only will this ensure the success of the project, but it will also provide\\n\\na better work environment for all.\\n\\nShip fast, sustainably: Software developers often focus on the idea of “shipping” a product. But that’s not the\\n\\nend goal. The goal is to have sustainable software engineering practices. This means shipping software in small\\n\\niterations, and ensuring that the team has a good work-life balance because nobody can be productive if they’re', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='exhausted or burnt out.\\n\\nFix problems, even when they’re not yours: Ther is an epidemic of engineers who feel the responsibility for their\\n\\nown code but not the code that they don’t touch. This leads to uncoordinated, confusing and unmaintainable code.\\n\\nIt is important for developers to take responsibility for the whole codebase and not just their own piece of it.\\n\\nCommunicate early and often: Communication is one of the most important skills in any team, and software de-\\n\\nvelopment teams are no exception. From simple voice conversations to a more complicated Google chat discussion,\\n\\nit is essential that all members understand what is going on at any given moment. Without good communication,\\n\\nchaos can ensue and deadlines can be missed.\\n\\nCompany (i.e. Client) over team over individual: The software development team should prioritize the client’s\\n\\nrequirements over the team’s ambitions and individual’s professional progress. It is easy to get distracted with new', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='“shiny” software and frameworks. However, when the client’s needs are not a priority, this can lead to poor code\\n\\nquality and even missed deadlines.\\n\\nOutcome over effort (Don’t reinvent the wheel): Most of the code required to process data, define a model and\\n\\nthe given model is available online. There is no need to write everything from scratch.\\n\\nReference(s):\\n\\nYou can find more engineering values and principles in this GitHub Repo\\n\\n2.4. What kind of engineering principles can I set for my development team?\\n\\n23\\n\\nAI Practitioner Handbook\\n\\n2.5 How might we simplify and translate technical jargon?\\n\\nContributor: Shafir Ahmad, Senior AI Technical Consultant\\n\\nCommunicating with people who are not actively involved in your project can present a challenge. Technical staff such as engineers and developers find it difficult to communicate complex technical concepts to clients, senior management and customers alike. It might even look like you are speaking different languages.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The fact is that many of these stakeholders you communicate with will understand the big picture and high level concepts, but not the intricate engineering details. It is better to keep these details for a more technically inclined audience.\\n\\nHere are some tips you can use to reach an understanding between yourself and non-technical stakeholders.\\n\\n2.5.1 1. Know your audience\\n\\nLearn about your audience. Find out what their knowledge is on the subject matter beforehand so that you can tailor the information to their needs. During early engagements, it is better to communicate at a level below what you reasonably expect them to know, while being careful not to dumb it down too much such as to cause offence. When in doubt, you could always let them know “If this is something you already know, let us know and we can skip to the next section.”', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Additionally, if the level of understanding is different between participants, it would be helpful to target the “least technical person” and while pre-empting the rest of the audience to “bear with me if you already know some of this information”. Do continually observe the stakeholder’s gestures, facial expressions and body language to ensure everyone is following along.\\n\\n2.5.2 2. Reduce or Eliminate Tech Jargon\\n\\nStart eliminating technical jargon and acronyms from your conversations with the stakeholders. Certain terms may not make any sense to the stakeholder, and others may hold a different meaning from the one you commonly use (e.g. NLP = Neuro Linguistic Programming vs NLP = Natural Language Processing).\\n\\nTry translating technical terms into their equivalents in everyday language. Pro tip: it helps if you understand the techni- calities deeply.\\n\\nHere is an example of communicating with a non-technical decision-maker:', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Technical jargon: \"We will ship an end-to-end pipeline for you to retrain the model␣\\n\\n↪upon data drift in your production system.\"\\n\\nEveryday language: \"We will share the code and instructions with your technical team␣\\n\\n↪to enable them to refresh the model after the solution has been implemented and you␣ ↪have gathered enough new data.\"\\n\\nConsider the use of metaphors, analogies and storytelling techniques that will help your audience understand the material.\\n\\n24\\n\\nChapter 2. Project Management & Technical Leadership\\n\\nAI Practitioner Handbook\\n\\n2.5.3 3. Provide background information, context and education', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Do not assume that the stakeholder knows or does not know certain concepts or terms, or that they understand the con- text. However, do assume that they are intelligent and will be able to understand if given an adequate explanation. Do translate key jargon and acronyms that are important to the discussion in a way that makes it simple for the stakeholder to understand. Metaphors and analogies are a good way to translate a technical concept into a more familiar context. It could be useful to give a glossary and definitions of some necessary terms.\\n\\n2.5.4 4. Use visuals to illustrate technical concepts', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='While written words and verbal speech are effective ways of communication, working to make the concepts visual can make it more effective. You may have heard the adage “A picture is worth a thousand words”. Research suggest that adding visuals can help increase memory recall to 65% versus 10% from just hearing it. With the use of creative illustrations such as flowcharts, process diagrams, demos, models and architecture diagrams, stakeholders can learn and understand the most technical concepts from these visual aids.\\n\\nTake care not to overload the visuals, and keep it as simple and uncluttered as possible. Remember to tailor it to your audience as a high level executive may not need an in depth understanding of an architecture diagram, but needs an overview.\\n\\n2.5.5 Conclusion\\n\\nPro Tip: An excellent way to practice is to try explaining some of your work to your spouse or friend. As the saying goes, “If you can’t explain it simply, you don’t understand it well enough.”', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='With just a bit of thought and planning, you can make the discussion more engaging and understandable for the partici- pants. You may need to arrange for additional meetings to provide the participants with the level of depth of understanding they require.\\n\\n2.5.6 References\\n\\nPower of Visuals\\n\\nKnow your Audience\\n\\nHow to give a technical presentation\\n\\nCut Technical Jargon\\n\\n2.5. How might we simplify and translate technical jargon?\\n\\n25\\n\\nAI Practitioner Handbook\\n\\n26\\n\\nChapter 2. Project Management & Technical Leadership\\n\\nCHAPTER\\n\\nTHREE\\n\\nCOLLABORATIVE DEVELOPMENT PLATFORMS\\n\\n3.1 Overview\\n\\nThis chapter consolidates sections on practices that facilitate a team of developers who are contributing jointly to a single codebase. This includes considerations on maintaining code quality, minimising code conflicts, and continuous integration.\\n\\n3.2 What are the key platforms required for collaborative ML devel-\\n\\nopment?\\n\\nContributor(s): Dylan Poh Guan Kiong, AI Engineer (GitHub)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='With the increasing complexity of machine learning projects, it is highly likely that a team of developers is required to complete the task. As a result, it is critical to have the ability to collaborate efficiently.\\n\\nFundamentally, there are two ways that teams collaborate.\\n\\nLuke Marsden: When you boil it down to basics, there are really two fundamental modes of collaborating between different people doing work: synchronous and asynchronous.\\n\\nSynchronous: Communications are scheduled, real-time interactions by phone, video, or in-person, sometimes interrupt- ing one another for urgent tasks or simply asking a question. In terms of development, the upstream have to complete their tasks before the downstream team can continue with their work (Dependencies issues)\\n\\nAsynchronous: Communication happens on your own time, with a different team or people concurrently working on a different task. An efficient mechanism is required to resolve merge conflicts.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='With this in mind, we seek to introduce some tools and practices for a team to get started in the hope that stumbling blocks will be reduced to the minimum. The practices introduced here assume development using Python as the primary language.\\n\\n27\\n\\nAI Practitioner Handbook\\n\\n3.2.1 Communication tools\\n\\nEstablishing effective communication right from the beginning of any project will allow all team members to align toward the same goal and reduce any possible friction. A quintessential example of miscommunication is an unclear problem statement leading to team members having different views on the outcome of the project.\\n\\nThe following tools (by no means exhaustive) may be useful for remote, office, and hybrid working arrangements:\\n\\nA big whiteboard often produces the most efficient channel to share ideas. (Physical Meetup)\\n\\nGoogle Workspace, a full suite of communication tools such as Google meet, shared Google calendar, and Gmail', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='to see the complete list of participants and scheduled meetings. Other option includes the Microsoft Team.\\n\\nMiro is an easy to use, intuitive, collobration tool that uses little effort to setup quickly. Other option includes\\n\\nCoggle.\\n\\n3.2.2 Code versioning\\n\\nJupyter notebooks (.ipynb) are powerful, but also have many limitations. For example, it is impractical to use pure git for versioning Jupyter notebooks (because of large amounts of embedded and frequently changing HTML). Jupyter notebooks are also unsuitable for deployment or production. Naming and renaming files or folders with comments are also undesirable and difficult to keep track of.\\n\\nInstead, using git for distributed version control and tracking changes helps coordinate work among team members. Developing code in Python script (.py) is highly preferred and using GitLab or GitHub to manage your code online with an advanced interface to resolve merged conflict that occurs frequently for asynchronous work.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Merge conflicts occur when the source and target branches of a merge request (as termed by GitLab) or pull request (as termed by GitHub) contain different changes or commits. As mentioned, this is especially common during collaboration when more than two engineers are contributing to the code. In this case, engineers must choose which change to accept. Once the changes are made, you can assign reviewer to inspect the changes which ensure the correctness of the merge and prevent breaking the code. Examples of the occurrence of and tools to handle conflicts are linked here for both GitHub and GitLab.\\n\\nKeeping a standardised environment for development keeps Python package dependencies consistent, and together with git, allows different members to easily switch between different machines such as local machines or machines in the cloud. One popular option that manages the environment is Conda (environment.yml)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='More in-depth information on setting up a machine learning project repository can be found here.\\n\\n3.2.3 Model/weight/data versioning\\n\\nShared storage such as cloud storage ensures the team is assessing the correct dataset and having access to a shared transformed dataset cuts down data preprocessing/transformation time allowing the engineer to focus on modeling. Also, weights and datasets should not be pushed to the version control system, as the repository size might become too large for the local machine.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Having a system to keep track of the trained model and the corresponding dataset early on will prove to be fruitful, as more experiments will make it difficult to trace back. One good practice will be to save the checkpoints and the models together and keep track of the corresponding dataset used for the training. Recommended tools include MLflow and Weights & Biases for tracking experiments and versioning models. DVC and neptune.ai are tools to consider for data version control. Even a simple shared spreadsheet keeping track of model/weights/dataset versioning will pay off eventually.\\n\\nIt is also possible to set up a cloud-based compute/development environment where all collaborators can share the same environment and tools (VScode, Jupyter Lab) and shared storage. Some of the examples are Amazon EC2 Instance Types, Data Science Virtual Machine and Google Compute Engine.\\n\\n28\\n\\nChapter 3. Collaborative Development Platforms', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='3.3 What are some considerations in setting up a project repository?\\n\\nContributor(s): Siti Nuruljannah Baharudin, AI Engineer\\n\\nAI Practitioner Handbook\\n\\n3.3.1 Use of existing project templates\\n\\nPrior to setting up a repository, you would want to plan the structure of your code base. If your organisation provides an existing template for machine learning projects, that would be a good place to start. Otherwise, you may want to consider using one of the many open-source frameworks available, which come with boilerplate code for the various components in a typical ML project. The scaffolding provided by such frameworks would not only save you some effort in setting up the project, but also help to establish a standard for developers to create modular and maintainable code.\\n\\n3.3.2 Configuration management', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Regardless of the framework or template used, management of configuration files is an important area of consideration in any ML project, since ML pipelines designed to be flexible and extensible would rely heavily on configuration files as inputs. You are recommended to keep all configuration files in a single directory, separate from the source code files. It is also good practice to maintain separate configuration files for each environment developers work in. For instance, configuration files for the local development environment could be placed in a subdirectory /config/local, files for the test server environment in /config/test, and so on.\\n\\n3.3.3 General file management', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In a Git repository, a .gitignore file is crucial in helping you filter out files and directories that should not be pushed to the repository, such as configuration files for a developer’s local environment, which would likely contain settings unique to the individual’s workspace. Other examples of files that should not end up in the repository are authentication keys, data files, log files and trained model weights.\\n\\nTypically, individual developers would create their own notebooks containing EDA-related code or prototype code for the models being explored. Although these may end up as throwaway code, they serve the useful purpose of knowledge sharing at the beginning of a project. It is recommended that these files are placed in a notebooks subdirectory and pushed to the repository within each developer’s personal branch, which do not need to be merged to the main branch.\\n\\n3.3.4 Dependency management', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Another area to consider is the management of code dependencies. At the start of a new project, initialising a single set of initial dependencies would suffice. In a Python project, this could mean maintaining a single requirements.txt file. As the code increases in complexity, or if separate components of the application are to be deployed in different environments, it is common for conflicting dependencies to occur. In this case, developers would need to maintain multiple sets of dependencies. These dependency files could be named with suffixes indicating the environment or module it is for, e.g. requirements-train.txt.\\n\\n3.3. What are some considerations in setting up a project repository?\\n\\n29\\n\\nAI Practitioner Handbook\\n\\n3.3.5 Developer productivity tools', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Besides software-related components, your repository may contain other files such as issue templates and hook scripts. Issue templates are used to pre-fill issue descriptions with custom fields or sections, which guides developers to include all necessary information when creating an issue. For instance, an issue template for a bug could include sections like “Steps to reproduce”, “Expected result” and “Actual result”.\\n\\nIn a Git repository, hooks are scripts that Git executes locally before or after events such as commit and push. For example, you could set up a pre-commit script to run code through a linter, ensuring it meets the standards before it is allowed to be committed. With the numerous hook scripts and frameworks available, hooks do not require a lot of effort to set up, yet can greatly increase developer productivity when used appropriately.\\n\\nReference(s):\\n\\nAI Singapore’s Cookiecutter Template for End-to-end ML Projects', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Kedro - A Python framework for creating reproducible, maintainable and modular data science code\\n\\nGit Branch tutorial\\n\\n30\\n\\nChapter 3. Collaborative Development Platforms\\n\\nCHAPTER\\n\\nFOUR\\n\\nLITERATURE REVIEW\\n\\n4.1 Overview\\n\\nThis chapter contains sections that aim to systematise the process of reviewing literature relating to a problem domain, as well as go-to references that provide a starting point for typical AI problems.\\n\\n4.2 What are some of the factors to consider during literature re-\\n\\nview?\\n\\nContributor(s): Andy Ong, AI Engineer\\n\\nLiterature reviews are necessary at the start of every AI project to understand and explore available solutions in the market. Business problems require different solutions, and below are some factors that you can consider while performing your literature review.\\n\\nBusiness needs\\n\\nDevelopment time\\n\\nExisting code respositories\\n\\nPre-trained models\\n\\nReported results\\n\\n4.2.1 Business needs', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Business considerations must be taken into account during any literature review. These considerations tend to set re- strictions on the project. Understanding these considerations can ensure that you are researching relevant solutions. For example, a medical AI project requires model explainability, and this will require a search for solutions involving ex- plainable models. Alternatively, if the use case requires edge deployment in mobile devices, this would instead direct the selection of literature toward those that support smaller and quicker models.\\n\\nThe literature search should not be limited to your project’s domain. An example is using autoencoders for anomaly detection even though they are typically used for computer vision. Such literature can introduce different approaches and highlight possible challenges that you might face in your project. You can understand the various approaches and their benefits, and how you can integrate them into your solution.\\n\\n31', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI Practitioner Handbook\\n\\n4.2.2 Development time\\n\\nAll projects have a set of timelines to adhere to, and development time may not be sufficient to build pipelines from scratch. You will have to allocate time for integration tests, project handover and other ad-hoc tasks such as troubleshooting. You can research and identify potential libraries to aid in your pipeline to avoid building it from scratch. You can also attempt to inherit certain classes from suitable libraries to customise them to your project with minimal code. The following sections are specific examples of strategies that can aid in shortening development time.\\n\\n4.2.3 Existing code repositories\\n\\nYou should be reading up on literature that include training or inference scripts in their code repository. These solutions can be incorporated into your codebase with ease as compared to solutions without any scripts. Papers with Code contains papers with open-sourced code repositories.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='If the literature is supported by code repositories, you should evaluate the readability of the code and complexity of inte- gration. Supplied code repositories can often be difficult to understand, and integration may take longer than anticipated. Additionally, some repositories/implementations may be more comprehensive than others, such as those that contain class weights as configurable hyperparameters or include an evaluation function.\\n\\nConsider the copyright agreements and licenses for the papers and codes in light of this. It’s possible that your project will not be able to use commercial IP software or certain licenses (like the GPL). Examples of permissive licenses with few limitations on use include the MIT and BSD licenses.\\n\\n4.2.4 Pre\\n\\n\\n\\ntrained models', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='For NLP or CV projects, it may be advantageous to search for solutions with pre-trained, open-sourced models. This will cut down on training time by ensuring that you can use transfer learning to such tasks. Long training times are needed for language or computer vision models, which can add time and cost to a project. It is advantageous if the pre-trained model was developed using datasets from a field related to your use case. For instance, if your use case involves evaluating financial accounts, FinBERT, a pre-trained NLP model, may be more immediately applicable than BERT-base.\\n\\n4.2.5 Reported results', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The results reported in the study should be interpreted in a discerning manner. Reported results may entail some cherry- picking. More crucially, the data used is likely different from your project’s use-case. Consequently, expect to see a difference in accuracy, since data is typically the main element affecting how well any model performs. Rather than focusing on the reported results, you should instead concentrate on the evaluation techniques and algorithms employed in the literature.\\n\\n32\\n\\nChapter 4. Literature Review\\n\\nCHAPTER\\n\\nFIVE\\n\\nDATA MANAGEMENT, EXPLORATION & PROCESSING\\n\\n5.1 Overview\\n\\nThis chapters encompasses sections with a focus on data engineering, data lineage, data versioning, exploratory data analysis and feature engineering.\\n\\n5.2 Which data storage options are suitable for the project?\\n\\nContributor(s): Syakyr Surani, Platforms Engineer', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='There are many data storage options that one can implement for their project, but to choose one (or several) of these options can be confusing to some. This guide hopes to give you some guidance in navigating various data storage options that are suitable for your project by honing in on three main considerations when choosing the right options for you.\\n\\n5.2.1 Requirements and Competency of Project Sponsor', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Data competency of the Project Sponsor cannot be understated when choosing a suitable data storage option, especially when they are not prepped up to be part of an AI-powered project. The skillsets of analysts in any given organisation can vary a lot from simple Excel data entry ones to highly automated processes employed by big tech companies. Therefore, it is important to understand the competency as well as the readiness of the Project Sponsor in committing on a set of data storage options. You can read more about this in our AI Readiness Index article as one of the ways to ascertain the Project Sponsor’s data competency.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='To give an example, you have a team from the Project sponsor that has only started going through AI tutorials, whether from YouTube or a reputable course provider. The data you need for the project is relatively small and only require the basic skills needed to build a model and to be used internally and sparingly. Then, using CSV or Excel files may be sufficient for the project.\\n\\nOn the other hand, if the project takes in a lot of data from multiple data sources, then they may want to implement a data lake to store those data before being processed and transformed into data warehouse(s). An article from Guru99 gives more insight regarding about data lakes and warehouses here.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='We would also need to take the willingness of the Project Sponsor to accept a solution that is outside their comfort zone into consideration as well. In some cases, they might request something that is more sustainable than their current solution, while other Project Sponsors may not be ready to handle databases and are only comfortable with what they have been doing.\\n\\n33\\n\\nAI Practitioner Handbook\\n\\n5.2.2 Reliability and Consistency\\n\\nReliability and consistency may be important in your project, especially when the latest dataset may be needed to reduce model drifting or other similar issues. Flat files such as CSV and Excel files may not be a viable solution as different members in your team may be using different versions of data, which may result in unreliable and inconsistent metrics when it comes to evaluation.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Therefore, it would be recommended to look at a more reliable solution such as database management systems (examples include SQL, MongoDB, etc.) if that is to be the case. A section provided by Microsoft Azure can give more information regarding these types of systems here, relational or non-relational.\\n\\n5.2.3 Data Management (Simplicity vs Complexity)\\n\\nThe topic of data management coincides with the competency of your team as well as how reliable you need the data to be in the manner of cost efficiency. This cost could be in financial terms, in terms of your team’s valuable time to pick up a new skillset, in terms of potential security and privacy issues surrounding the use of self-hosted or managed cloud solutions, or a mixture of all three.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Does your organisation have the capability to resolve operational issues to any data store solutions proposed? Is the data sensitive enough to warrant an in-house solution, or is it more cost-effective to have it run in one of the many managed cloud providers instead? Or is it less hassle to just share flat files such as CSV and Excel files in a cloud storage? These questions could be used to shape your current project’s requirements, but you should also project those requirements towards the long term, and discuss within your team to reduce the times needed to refactor your project.\\n\\n5.2.4 Final Thoughts\\n\\nThere are more considerations you may need to take note of other than the three that is discussed in this guide, but we believe that these three are the main considerations to take note of. However, if you feel that you need more tips on navigating your way on finding a suitable data storage solution, there is a Microsoft Azure page you might find more insights here.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='5.2.5 References\\n\\nAI Readiness Index | AI Singapore\\n\\nData Lake vs Data Warehouse: What’s the Difference? | Guru99\\n\\nUnderstanding Data Store Models | Microsoft\\n\\nCriteria for Choosing a Data Store | Microsoft\\n\\n5.3 Is there a systematic structure for performing exploratory data\\n\\nanalysis?\\n\\nContributor: Er YuYang, Senior AI Engineer\\n\\nThe purpose of this article is to have a guide to performing exploratory analysis (EDA) in a more systematic manner. This guide is designed for a generic EDA purpose and primarily focuses on tabular data. However, parts of this workflow are still applicable for unstructured data types.\\n\\n34\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\n5.3.1 Overview\\n\\nMost EDA activities should contain these blocks:\\n\\nStructure investigation', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This section should contains the most basic checks on the overall completeness of and level of information in the dataset. These include checking the data size and type, as well as granularity/cardinality of each table. It should also examine if the data contains any private or sensitive data.\\n\\nContent investigation\\n\\nThis section looks into the quality, distribution and relationships in the dataset, and need not be in this particular order.\\n\\nQuality checks\\n\\nThis sub-section should cover aspects of data quality such as missing and erroneous values. The main aim is to identify potential data to drop, repair or impute.\\n\\nDistribution checks\\n\\nThis sub-section should cover the understanding of each individual feature. The main aim is to find out the chracteristics of each feature, and also to uncover trends or patterns in the data.\\n\\nRelationship checks', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This sub-section should cover more in-depth analyses such as bivariate relationships between features, and between fea- tures and target. The main aim is to generate ideas for feature engineering and selection based on observed relationships.\\n\\n5.3.2 Structure investigation\\n\\nThis section should be short (about less than 10 notebook cells), as it only aims to gain some insights on the size and datatype of the data. This sections does not go into the content of each variable/feature.\\n\\nWhen checking the datatype, organise features according to whether they are numeric/continuous or categorical. This will help to decide on suitable visualisations when performing content investigation later.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='When dealing with private or sensitive data, discuss with stakeholder whether masking or removing them is a better option. When duplicate rows are present, highlight them and discuss with stakeholders on how to deal with them. Inappropriate handling of duplicate rows can results in data leakage when performing data split for model training/evaluation.\\n\\nWhen performing granularity/cardinality checks, it is important to understand how fine or coarse each row and column is and verify whether the table is of the correct granularity per the database schema\\n\\nExpected ouputs of structure investigation:\\n\\nRaw version of data loaded in the notebook\\n\\nSize of data, whether parallel processing is needed\\n\\nDatatype of features, which features are numeric/continuous, or categorical\\n\\n5.3.\\n\\nIs there a systematic structure for performing exploratory data analysis?\\n\\n35\\n\\nAI Practitioner Handbook\\n\\n(Optional) Further discussion with stakeholders on handling private/sensitive data and duplicate rows', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='5.3.3 Content investigation\\n\\nQuality checks\\n\\nThis sub-section should cover matters relating to data cleaning and imputation. If there are too many missing values, consider dropping the column. However if dropping is not an option, a more sophisticated method for imputing the missing value is to use the other avaiable features to predict the value of that particular column (model-based imputation/multiple imputation).\\n\\nThe choice of approach for handling missing data is also influenced by the type of “missingness”. There are mainly three kinds of “missingness”: MCAR, MAR, MNAR (see article link at reference section). Before performing any imputation, it is highly recommended to first consult the stakeholder and understand possible reasons behind the “missingness”.\\n\\nGenerally, there are 4 ways to impute:\\n\\nAssign a fixed value(s) based on domain knowledge\\n\\nSimple imputation based on statistical summaries\\n\\nModel-based imputation\\n\\nMultiple imputation (see video link at reference section)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Expected ouputs:\\n\\nCleaned version of data ready for plotting charts\\n\\nIdentify data/column(s) that requires preprocessing in the datapipeline\\n\\nDistribution checks\\n\\nThis sub-section should cover the generation of distribution and summary statistics. Understanding properties like central tendency (mean), spread (variance) and skewness help you to spot erroneous values and potential outliers easily.\\n\\nIf there are district groups in the data, try grouping them and perform individual analysis for each group, in addition to the analysis for the entire dataset. Try to use looping to cover the entire possible combinations when plotting charts instead of selecting particular features combination that you preceive is useful. Investigate further if certain combination of features have interesting results.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Perform cardinality checks on categorical features, as there could be similar category due to spelling mistake or variations. Cleaning these erroneous categories will help to reduce unnecessary noise in the analysis. It also helps to assess whether some categories are too sparse and may need to be merged for greater statistical power.\\n\\nTry to link insights from the analysis to feature engineering. For example, in a parcel delivery context, if you find that number of deliveries (the target) is much higher on Mondays compared to the rest of the week, you could engineer a separate feature is_monday to boost the model’s performance.\\n\\nHighlight the findings on the notebook regardless of whether they are useful. This allows the reader to understand the purpose of the charts.\\n\\nThe analysis should cover the following:\\n\\nSummary statistics of the features\\n\\nDistribution of the features\\n\\nFeature patterns (trend)\\n\\nExpected ouputs:\\n\\n36\\n\\nChapter 5. Data Management, Exploration & Processing', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI Practitioner Handbook\\n\\nCharacteristics of each feature\\n\\nSpotting potential outliers and erroneous values\\n\\nSummary of the findings\\n\\nPotential feature characteristics that can be used for feature transformation in the data pipeline\\n\\nRelationship checks\\n\\nThis section aims to find out the relationship between feature and target so as to generate ideas for feature engineering and selection (feature importance). The goal is to identify correlated features or categories. Correlated features distort interpretability and feature importance. As such, it is worth exploring dropping or merging collinear features based on domain knowledge.\\n\\nIn the scenario if there are too few features in the data, feature engineering should be explored. Conversely if there are too many features, feature importance should be explored and then dropping less important features.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='If certain features have a very similar relationship to the target and are sparse, you could merge them during feature engineering to create a stronger feature. For instance, capital_gains and capital_loss can be merged into is_investor since if there are not many individuals who invest.\\n\\nThe analysis should cover the following:\\n\\nRelationships between each feature and the target\\n\\nRelationships between features\\n\\nExpected ouputs:\\n\\nIdentify features that are positively/negatively correlated to each other\\n\\nIdentify features that are positively/negatively correlated to the target label\\n\\nSummary of the findings\\n\\nIdeas on feature engineering/selection\\n\\n5.3.4 Automated EDA tools\\n\\nPandas Profiling and Google Facets are two excellent tools for performing EDA. They are useful to provide initial insights before drilling down into areas of interest through manual EDA.\\n\\n5.3.5 References\\n\\nEDA structure\\n\\nTypes of missingness\\n\\nMissForest\\n\\nVisualising distributions in seaborn', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Bivariate analysis in Python\\n\\nPhi K correlation coefficient\\n\\nMultiple imputation (video)\\n\\nPandas Profiling\\n\\nGenerate reports using Pandas Profiling\\n\\n5.3.\\n\\nIs there a systematic structure for performing exploratory data analysis?\\n\\n37\\n\\nAI Practitioner Handbook\\n\\nVisualising ML datasets using Google Facets\\n\\n5.4 What are some ways to do EDA for CV tasks?\\n\\nContributor: Calvin Neo, AI Engineer\\n\\nThe purpose of this article is to guide new AI Engineers on performing Exploratory Data Analysis of Computer Vision tasks in a systematic manner. It assumes a basic understanding of image format, and how bounding boxes and masks are drawn.\\n\\n5.4.1 Image EDA\\n\\nBasic Image Analysis\\n\\nThe basic image analysis considers the broad characteristics of the images that have been obtained.\\n\\nThe guiding questions are as such:\\n\\n1. What is the number of images provided? Is this the number of images promised to be delivered for experiments?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='2. What is the format of images eg JPG/PNG/JPEG? If there are multiple formats given, what are the counts for each\\n\\nimage format?\\n\\n3. What are the image sizes/resolution (eg 1080x960)? If there are multiple dimensions, what are the counts for each\\n\\ndimension?\\n\\n4. How similar are the images to each other? You can do an EDA to check on this by performing image hashing to discover groups which are similar to each other. This also helps to uncover whether some of the images might have been curated from a continuous video stream of a scene that is changing very slowly - in such a case, these images may be near-duplicates.\\n\\nColour Intensity of Images', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The colour intensity of images can be explored by either mean or mode of the channel values of each image. The goal for analysing annotations’ colour channels is to understand what the dominant colours are. For example, images in forest settings may have more green, and images along water bodies are expected to be blue-dominated. This also ties in with annotations, where if the annotated objects are similar in colour values, the model would need to rely on feature other than colours such as edges.\\n\\nBrightness of Images\\n\\nBrightness values are closely related to colour intensity. If the colour intensity for all channels are higher, then the images may appear to be brighter than normal and vice versa. It is thus the job of the AI Engineer to explore the data to ensure the brightness values of the images are of the normal range. Otherwise, they would need to scale up or down the brightness levels to ensure that the images for training are consistent.\\n\\n38', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Chapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\nContrast of Images\\n\\nContrast is the pixel to pixel difference in channel values. For example, a green frog in Bukit Timah Nature Reserve would be said to have low contrast. This makes edge detection more important for the model.The onus is thus on the AI Engineer to explore the images to ensure the contrast levels are of the normal range. Otherwise, they would need to employ contrast adjustment techniques to ensure contrast values are normalised.\\n\\nImage Similarity\\n\\n1. PCA or ISOMAP per channel can be performed on the images to see how the images are related to each other.\\n\\n5.4. What are some ways to do EDA for CV tasks?\\n\\n39\\n\\nAI Practitioner Handbook\\n\\n5.4.2 Annotations (Basic Overview)\\n\\nBounding Boxes Formats\\n\\n1. COCO\\n\\n\\n\\n[x_min, y_min, width, height]\\n\\n2. YOLO\\n\\n\\n\\n[x_center, y_center, width, height]\\n\\n3. Pascal VOC\\n\\n\\n\\n[x_min, y_min, x_max, y_max]', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='For example, if the given format is in Pascal VOC, and the model requires it in YOLO format, we would need to convert the annotations to YOLO format for training.\\n\\nAn example for during inference: Read images using CV2, run YOLOv3 model, convert and return in the desired format according to what is required to be delivered.\\n\\nMasks Format\\n\\nMasks annotations gives the pixel-wise positions of the mask.\\n\\nMasks are provided in an array of x-y pairs. It will typically be:\\n\\nmasks_per_instance = [x_1, y_1, x_2, y_2, ... x_n, y_n]\\n\\nMasks may also come in the form of PNG files, where the PNG’s image size would correspond to the original image size.\\n\\n5.4.3 Annotations EDA\\n\\nBasic Annotations Analysis\\n\\nThe basic annotations analysis attempts to find characteristics of the annotation such as number (and proportion) of classes as well as the values of the bounding boxes/ masks.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This is useful because it helps determine what type of metrics should be used, how the data (images) should be split, and what parameters could be tuned. For metrics, the usual case for this is error analysis of model performance on bounding box sizes. For example, if the bboxes/masks are mostly large objects among all the images, assuming images are sufficiently clear, it is clear that the model will do well. However, if the annotations are small, then the model will inevitably struggle. Hence, the AI Engineer will pay attention to certain metrics more, such as mAP with Area=Large.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The data split refers to how one might partition the data for training/validating/testing the model. Generally, all sets would need similar proportions of the bbox size (small/medium/large) and class distribution. In the ideal scenario, these proportions are easy to split at random, but it is extremely rare. Thus, by performing EDA on the annotations, the Engineer will have a better idea on how the data split can be done.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Tuning parameters refers to configuring parts of model training/inference that could be useful for better performance. An example of this is anchor box size(s). By default, the anchor box sizes in the models were set to be tuned to the dataset that they were benchmarked against, for example MS COCO Dataset. However, the bbox/mask of real data that is received would not fit nicely to these anchor box sizes. Hence, by understanding a particular range of typical bbox/mark sizes, the anchor box sizes can be tuned to accomodate the range. This has been observed to be beneficial to model training and inference performance.\\n\\nWith these considerations in mind, these are some guiding questions when performing EDA on annotations:\\n\\n1. How many instances of each classes are there?\\n\\n2. Bounding Box - what is width and height distribution overall and per class?\\n\\n3. Bounding Box - what is the overall size (W*H) overall and per class?\\n\\n40\\n\\nChapter 5. Data Management, Exploration & Processing', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='4. Bounding Box - What are the top few Width/Height combination? This can be found using clustering methods.\\n\\n5. Segmentation - what is distribution of the mask pixel counts overall and per class?\\n\\nAI Practitioner Handbook\\n\\nColours of Annotations\\n\\nAs with images, the annotations have 1 or more channels per pixel, with the most popular format being Red, Green and Blue (RGB). The utility for this mainly lets the researchers know if the model would be better served trying to use certain channels when making inference.\\n\\nAverage Intensity per channel\\n\\nTo explore this for all classes, we can firstly average the colour intensity of the annotation’s pixels per channel.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Secondly, countplot of the average colour intensity values could be generated. The image below shows the counts of each annotation with the average values of the red channels separated by classes. In this case, there are three classes, with one of the class having very spread out red channel intensity, and the other two classes having colour intensity being normally distributed.\\n\\nExample of a Height-Width comparison among bboxes separated by class.\\n\\nMode Intensity per channel\\n\\nBesides averaging the intensity per annotation, the highest occurring value (mode) of the intensity of the annotation can also be used to analyse the colour of the annotation.\\n\\n5.4. What are some ways to do EDA for CV tasks?\\n\\n41\\n\\nAI Practitioner Handbook\\n\\nThere are many other ways to visualise this. The main goal is to check on the colour distribution among the annotations.\\n\\n5.4.4 Annotations Random Sample Checks\\n\\nBounding Box and Mask Random Sample Checks', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Bounding Boxes Random Sample Check refers to performing a check by sampling images and analysing their annotations. These try to account to account for human error that very often occurs.\\n\\n1. Are the bboxes/masks drawn tight enough?\\n\\n2. Were the bboxes/masks drawn accurately?\\n\\n3. Are the objects being occluded? In other words, were parts of the object blocked by another object? In general, if the object of interest is being occluded, its annotation should still be drawn such that it includes the occluded portion.\\n\\n4. Are the objects truncated? In other words, were parts of the objects “spilling” out of the image?\\n\\nFor occlusion and truncation, it is recommended to include “occlusion” and “truncation” in the metadata of the annotation.\\n\\n5.4.5 Conclusion', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This was a short survey of some of the areas in which the AI Engineer can begin in exploratory data analysis for images as well as their annotations. This is by no means exhaustive, and the AI Engineer should explore further methods for EDA in this space.\\n\\n5.4.6 References\\n\\nPerceptual Hashing\\n\\n\\n\\nhttps://www.phash.org/\\n\\nVOC annotation guide - http://host.robots.ox.ac.uk/pascal/VOC/voc2011/guidelines.htmlcd The Scientist and Engineer’s Guide to Digital Signal Processing - https://www.dspguide.com/ch23/5.htm\\n\\nFurther EDA readings\\n\\nHistograms:\\n\\nhttps://homepages.inf.ed.ac.uk/rbf/HIPR2/histgram.htm\\n\\nhttp://web.cs.wpi.edu/~emmanuel/courses/cs545/S14/slides/lecture02.pdf\\n\\nhttps://www.allaboutcircuits.com/technical\\n\\n\\n\\narticles/image\\n\\n\\n\\nhistogram\\n\\n\\n\\ncharacteristics\\n\\n\\n\\nmachine\\n\\n\\n\\nlearning\\n\\n\\n\\nimage\\n\\n\\n\\nprocessing/\\n\\nAnchor Boxes:\\n\\nhttps://d2l.ai/chapter_computer\\n\\n\\n\\nvision/anchor.html\\n\\nOther Data Exploration Ideas:\\n\\nhttps://neptune.ai/blog/data\\n\\n\\n\\nexploration\\n\\n\\n\\nfor\\n\\n\\n\\nimage\\n\\n\\n\\nsegmentation\\n\\n\\n\\nand', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='object\\n\\n\\n\\ndetection\\n\\n42\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\n5.5 What are the various data split strategies?\\n\\nContributor(s): Lee Xin Jie, Senior AI Engineer (100E)\\n\\nAI Practitioner Handbook\\n\\n5.5.1 Static train\\n\\n\\n\\ntest Split\\n\\nThe most common train test split strategy is to create static training, validation and test data sets.\\n\\nTrain set\\n\\nThe train set is the subset of the data that will be used to train the ML model. The model’s performance on the train set will only provide an indication of how well the model will perform on previously seen data.\\n\\nValidation set', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The validation set, also commonly referred to as the development set, consists of data that is unseen during model training. It provides an estimation of the model’s performance during production. The validation set is often used during model hyperparameter tuning and algorithm selection. When performing algorithm selection, it is important to ensure that the validation set used is the same across all algorithms to ensure a fair comparison.\\n\\nTest set', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The test set is the second hold-out set that will not be used during training. Since the validation set is commonly used to influence the modelling decisions, our ML model pipeline is likely to be biased in performing well on the validation set. Hence, we will need a completely unseen set of data to provide a final benchmark of our model’s performance on unseen data. It is important to not use this test set to influence your modelling decision. Otherwise you will get an inflated sense of how well the model will perform in real life. You should only evaluate your model on the test set during the final step of the pipeline. As such, it is common for the train set performance to be higher than the validation performance, which in turn, should be higher than the test performance.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Tip: After finding the best model from validation, it is also good practice to re-run this model (with the best hyperpa- rameters) on the full dataset (train + validation + test) and save the resulting weights for further use.\\n\\nSplit ratio\\n\\nThere is no single ‘golden’ data splitting proportion. In general, the larger your dataset, the lower the proportion of data you have to set aside for the validation and test sets. If you have a dataset with a billion data points, you could theoretically set aside 1% of the data for the test set, and still have a sufficiently large test set of 10 million data points. Conversely, the smaller your dataset, the larger the proportion of data that should be set aside for your validation and test sets. This will allow you to have a more confident estimation of model performance during production, as your model’s performance will be determined on a larger and more diverse validation/test set.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='One commonly used rule of thumb will be to adopt a 70-20-10 split for your train, validation and test sets respectively. Ultimately, you should prioritise selecting a ratio catered to your needs.\\n\\n5.5. What are the various data split strategies?\\n\\n43\\n\\nAI Practitioner Handbook\\n\\n5.5.2 Cross\\n\\n\\n\\nvalidation\\n\\nA single, static validation set could potentially present a biased assessment of model performance. This is particularly the case with smaller datasets where favourable validation performance may arise by chance.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Cross-validation is an alternative to having a static validation set. In cross-validation, you will conduct multiple rounds of model training, each time with a different section of your dataset serving as the validation set. This will minimise the variance associated with choosing any particular choice of validation set. However, cross-validation does come with the penalty of additional training time. The difference could be significant especially when training large models. So it is usually recommended when you are dealing with a smaller dataset where there could be insufficient samples to form a reliable validation set. In such cases, favourable validation performance may arise by chance due to a higher proportion of ‘easier’ examples appearing in the validation set, hence the need for cross-validation.\\n\\nAn illustration of this is as follows.\\n\\nSource: Cross\\n\\n\\n\\nvalidation (statistics)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='To obtain the final evaluation result, you can take the average of all splits’ test set results. If you are satisfied with this result, you can train the final “champion” model by fitting the model to the entire dataset. This will be the model used for inference.\\n\\n5.5.3 3. Nested cross\\n\\n\\n\\nvalidation\\n\\nWhile cross-validation reduces the amount of overfitting as compared to the use of a static train test split, it does not reduce it completely. This is because the same score is used to select the best model and to evaluate the model.\\n\\nTo overcome it, you may choose to use nested cross-validation, which will separate the score used to select the best model and to evaluate the model. In this approach, the outer loop is used mainly for model evaluation, while the inner loop is used for hyperparameter tuning.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='To illustrate this, let us assume you are using an outer fold of 3, and an inner fold of 3, and you are tuning and evaluating for the parameter n for a model. For the outer loop, you will split the dataset into 3 folds, and rotate the test fold for each set.\\n\\n44\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\nTaking the first training set, you will further split it into 3 folds with 1 fold serving as the validation set.\\n\\nYou will train a model with a value of n, say n=1, on the training folds, and evaluate the model on the validation fold. You repeat the model training with the same n=1 value each time you rotate the validation fold. The mean of the scores across all validation folds will be the validation score for the parameter n=1. You will repeat this procedure for all other n values, and then select the best n value.\\n\\n5.5. What are the various data split strategies?\\n\\n45\\n\\nAI Practitioner Handbook\\n\\nTrain\\n\\nInn2,Inn3\\n\\nInn1,Inn3\\n\\nInn1,Inn2\\n\\nInn2,Inn3', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Inn1,Inn3\\n\\nInn1,Inn2\\n\\nTest\\n\\nInn1\\n\\nInn2\\n\\nInn3\\n\\nInn1\\n\\nInn2\\n\\nInn3\\n\\nn Val Acc\\n\\n92\\n\\n1\\n\\n90\\n\\n1\\n\\n88\\n\\n1\\n\\n80\\n\\n2\\n\\n83\\n\\n2\\n\\n77\\n\\n2\\n\\nn Mean Val Acc\\n\\n1\\n\\n2\\n\\n90\\n\\n80\\n\\nFinally, you will train on the full training and validation set with this best n parameter, and evaluate on the test set that was set aside initially in the outer fold.\\n\\nYou repeat the same procedure for all iterations of the outer loop, to obtain the final evaluation of the model.\\n\\nTrain\\n\\nOut2,Out3\\n\\nOut1,Out3\\n\\nOut1,Out2\\n\\nMean test accuracy: 88\\n\\nTest Best n Test Acc Out1 Out2 Out3\\n\\n92\\n\\n84\\n\\n88\\n\\n1\\n\\n2\\n\\n1\\n\\nWhen you are satisfied with the results, you can train the final “champion” model by fitting the model to the entire dataset.\\n\\nThe drawback of the nested cross-validation approach is that the training time will even be longer than the standard cross-validation.\\n\\nTip: In cases of extremely small datasets, you could consider nested cross-validation to maximise the usage of the whole dataset for training, validation and testing.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='5.5.4 Stratified Split\\n\\nFor datasets with unbalanced distribution of targets and/or features, you may want to consider stratified splitting. Stratified splitting aims to split your dataset, while maintaining similar proportions of any desired features/targets across your train, validation and test sets.\\n\\nAn example of a dataset with features that are skewed in their distribution will be a credit worthiness classification problem, where you may have fewer individuals in the dataset with ages of less than 20 years old. A random split could result in insufficient numbers of them getting assigned to the validation and test sets. If it is critical to model the behaviour of this age group, you can stratify the dataset based on age buckets. Doing so would fix the proportion of each age group getting assigned to each set.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='An example of a dataset with imbalanced targets will be a fraud detection dataset, where fraudulent examples are typically the minority. In this case, you will stratify the dataset based on the target.\\n\\nIn general, the larger the dataset, the less likely features and targets will be unevenly distributed across the sets. You should still check your dataset for any uneven distribution in features and targets.\\n\\n46\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\n5.5.5 Temporal Split\\n\\nWhen you are dealing with problems related to forecasting future values, you may want to consider temporal splitting. As an example, assume you have data from January to April. You may want to set aside data from January to February for your training dataset, March for your validation dataset, and April for your test dataset.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In fast moving environments such as fraud detection and cyber attack classification, bad actors might develop new fraud and cyber attacks techniques. It may be necessary to continuously train the model on the latest data to predict future frauds and cyber attacks, even though the task does not involve forecasting. Evaluating the model on historical frauds and attacks may be insufficient. Hence, you may choose to consider temporal splitting in this scenario.\\n\\nIn scenarios where there are high correlations between successive times, such as in weather forecasting, you will also want to consider temporal splits and avoid placing February 2 in the training set and February 1 in the validation set to minimise data leakage.\\n\\nFor seasonal data, you may want to take seasonality into account when performing temporal splits. As an example, you can place the first 20 days of every month into the training set, the next 5 days into the validation set, and the last 5 days in the test set.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='When performing cross validation for temporal data, you cannot simply use random data sampling to assign data points to the training and test sets. This is because there are temporal dependencies between each observation, and you generally want to avoid using future data to predict past data.\\n\\nFor each kth split in temporal cross validation, the first k folds will be assigned to the train set, while the k+1^{th} fold will be assigned to the test set. Hence, all successive training sets are supersets of previous sets.\\n\\nAn illustration of this is as follows.\\n\\nSource: Cross Validation in Time Series\\n\\nThe steps to obtain the final evaluation results are the same as cross validation. You will likewise obtain the final result by taking the average of all k splits’ tests results.\\n\\nSimilarly, when you are satisfied with the results, you can train the final “champion” model by fitting the model to the entire dataset.\\n\\n5.5. What are the various data split strategies?\\n\\n47', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI Practitioner Handbook\\n\\n5.5.6 References\\n\\nBuilding Machine Learning Powered Applications: Going from Idea to Product\\n\\nMachine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and\\n\\nMLOps\\n\\nMachine Learning Yearning: Technical Strategy for AI Engineers, In the Era of Deep Learning\\n\\n5.6 How do we make data splits repeatable?\\n\\nContributor(s): Lee Xin Jie, Senior AI Engineer (100E)\\n\\nIt is common for ML beginners to adopt naive random data splitting. In this approach, you may randomly select 80%, 10%, 10% of the data for your training, validation and testing data each time you run the algorithm. However, naive random data splitting is often not repeatable. While you may be able to fix random seeds to ensure some repeatability in data splitting on your machine, this repeatability is not guaranteed when the dataset has changed, or when your colleagues run the same data split algorithm on a different machine.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Having a repeatable train test split ensures that your experiments are reproducible not just by yourself, but by others.\\n\\nModel reproducibility is discussed in detail later in the section How can I maximise model reproducibility.\\n\\nFirstly, let’s introduce the concept of hashing.\\n\\nHashing is a one way transformation of any given string or key into a hash value, which is often an output of a fixed length. A hash function is always deterministic, and the same input will always generate the same output. Since it is a one way transformation, you are not able to retrieve the original string or key from the hash value.\\n\\nOne approach to ensure the data splits are repeatable is to take a selected set of column/columns and hash it. You can then make use of the hash to split your data. This hash value will always be the same every time you run the algorithm.\\n\\nColumns that are candidates for hashing should:\\n\\n1. Not be features or labels that are used for training\\n\\n2. Not be correlated to label', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='3. Be granular enough\\n\\nFeatures and labels should not be used for hashing, as doing so may inadvertently introduce bias into the data splits. This is because data with certain features or labels may appear more in some data splits, and less in the other data splits. Columns selected for hashing should also be granular enough, hence columns such as year should not be selected.\\n\\nExamples of columns that could be used are ID columns. Alternatively, you can concatenate all columns as a string and hash on that.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Once the column/columns are hashed, a simple approach for performing train-test split will be as follows. As an example, if you want 80% of your dataset to be allocated into the training set, 10% to the validation set and 10% to the test set, you can apply the modulo operator to the hashed value. Data points with remainders of less than 8 can be assigned to the training set. Those with remainders of 8 can be assigned to the validation set and those with remainders of 9 can be assigned to the test set.\\n\\nDate\\n\\n10 November 2022 ABC\\n\\n10 November 2022 XYZ\\n\\n11 November 2022 ABC\\n\\n12 November 2022 DEF\\n\\nStore Hash(Date,Store) % 10 Data Split\\n\\n6\\n\\n8\\n\\n3\\n\\n1\\n\\nTrain\\n\\nValidation\\n\\nTrain\\n\\nTrain\\n\\n48\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\nTo do this in python, an example will be:\\n\\ns = \"10november2022abc\" hashed_value = hash(s) remainder = hashed_value % 10', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='If there is a combination of columns that determine if 2 data points are correlated, and you want these data points to be grouped together in the same split, you can concatenate these columns together in a string and hash it.\\n\\nRepeatable data splitting is not just limited to the above mentioned method. The key thing is to ensure the repeatability of the splitting process by any users on any machines.\\n\\nFor time series problems, you may want to adopt temporal data splits. You can adopt various data splitting approaches as long as you version control the data split logic to ensure its reproducibility.\\n\\nExamples of applicable data splits are:\\n\\n1. Select cutoff datetimes for data split, e.g. data before 2022-12-15 00:00:00.000000 to be in the training set, data\\n\\nafter 2022-12-15 00:00:00.000000 to be in the validation set, etc.\\n\\n2. Selecting the first 10 months of every year to be in the training set, 11th month to be in the validation set, and the\\n\\n12th month to be in the test set.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='3. Selecting the first 20 days of every month to be in the training set, the next 5 days to be in the validation set, and\\n\\nthe last 5 days to be in the test set.\\n\\n5.6.1 References\\n\\nMachine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and\\n\\nMLOps\\n\\n5.7 What are some scenarios of bias, unfairness, data leakage in data\\n\\nsplits?\\n\\nContributor(s): Lee Xin Jie, Senior AI Engineer (100E)\\n\\n5.7.1 Potential scenarios of bias and unfairness in data splits and suggestions to\\n\\nremedy\\n\\nHaving biases in datasets may lead to unfairness in our models. We want to ensure that model predictions are fair for different groups of users and scenarios.\\n\\nThere are 2 sources of such bias and unfairness:\\n\\ni) The full dataset is skewed to certain subgroups (eg. we are training a model to predict voting behaviour in\\n\\nSingapore but the dataset contains 95% Chinese).', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='ii) The splitting of the full dataset into train, validation and test sets introduces skewness (eg. the train set contains\\n\\n30% aged above 60 while the validation set contains 15% aged above 60).\\n\\n5.7. What are some scenarios of bias, unfairness, data leakage in data splits?\\n\\n49\\n\\nAI Practitioner Handbook\\n\\nThis article discusses point ii).\\n\\nPoint i) is pertains to curating a representative dataset at the start of the project. This is partly discussed in the section What are the key areas to look out for in the data when framing the AI project?.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Certain problems are inherently imbalanced. For example, fraud detection datasets usually have fewer fraudulent examples than normal examples. In such a scenario, you should favour a stratified data split over a random split to ensure that there are equal proportions of fraudulent to normal examples in each data split. You will want to avoid a situation where your train set is predominantly normal, and your development set has a higher proportion of fraudulent data than in reality.\\n\\nWhen dealing with datasets involving geographical and demographical slices, you may also want to consider stratified data split to ensure that each split contains a balanced set of examples across geographical and demographical attributes. This will help reduce any potential unfairness in model predictions between users from different geographical and demo- graphical slices.\\n\\n5.7.2 Potential scenarios of data leakage in data splits and suggestions to remedy', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Data leakage is the scenario where during training, the model has access to information that will not be available during production. Often, if your model performs surprisingly well on the validation/test set (e.g. 100% test set performance), you should perform a check for data leakages. Data leakages should be avoided, as they allow the model to leverage on leaked information during training and achieve inflated performance on the validation/test set.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='A common example of data leakage is temporal data leakage, which may occur when you are building a model to forecast future events based on past events. For example, you are building a model to predict the price of a stock in future, and you have past daily data of the stock’s prices. If you perform a random data split, it is likely that the train set may contain future stock prices, while the test set may contain past stock prices. This will allow the model to leverage on future stock prices it encounters during training to predict past prices on the test set, thereby inflating the model’s performance.\\n\\nIn temporal problems involving high correlations between successive times, you should also aim to group successive days together in the same data split to minimise data leakage.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Datasets with duplicated data points are also likely to cause data leakages. If a random data split is adopted, you may have the same data points in both the train and test set. This will result in inflated test performance.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Datasets with near duplicates can also subtly introduce data leakage. One example will be in computer vision, where it is possible to have multiple images that are near duplicates but not perfect duplicates. This can easily occur when the data comes from video streams made up of continuous sequences of images. For example, if you are training a model to classify car models, you may have images of the exact same car taken 1 second apart. In this case the images are likely to have very similar characteristics. If you split the dataset randomly, you are likely to split these images into each of the train, validation and test set. Your model will then likely evaluate well on the images assigned to the test set, since it has already encountered similar images during training. In this scenario, you should consider a stratified splitting of your dataset, where images of the same car should be assigned to a single dataset split. You could use timestamps or image hashing to achieve this.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='When engineering features, ensure that you do not inject data leakage during the process. If you are building a model to predict how many items a user will purchase at a store on a particular day, you will want to only engineer features that are available during inference in production. Examples of features leveraging information that won’t be available are the number of items sold in the store during the same day, or a moving average of the number of items purchased by the user up to and including the day itself, etc.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Having a repeatable data split algorithm is important to avoid introducing unforeseen data leakages during model retraining. Imagine the scenario where an unrepeatable random data split algorithm is adopted. When you have new datasets, you will likely generate new data splits and trigger model retraining. After retraining, you will often have to compare the performance of the old model vs the new model, to choose which model to deploy. An unrepeatable data split algorithm may mean that some of the old training data may fall into the new validation dataset. Hence, your old\\n\\n50\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\nmodel may appear to perform better on the new validation dataset and you will deploy that model. However you may soon realise that the actual model performance in production is lower after deployment.\\n\\n5.7.3 References\\n\\nBuilding Machine Learning Powered Applications: Going from Idea to Product', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and\\n\\nMLOps\\n\\n5.8 How do I build a basic end-to-end workflow?\\n\\nContributor(s): Syakyr Surani, Platforms Engineer & Ryzal Kamis, Assistant Head (MLOps)\\n\\nThis guide assumes that you know basic data engineering and ML/DL modelling, and you want to start to build a workflow so that you have a working AI-powered product at the end of the project.\\n\\n5.8.1 What Does It Mean to Have an End-to-End Workflow\\n\\nWhen you started learning about ML, most likely you were only taught to train a model or perhaps to clean the data before training. Having an end-to-end workflow means you have a birds-eye view on the whole project from conception to deployment. This means you can delegate tasks better and diagnose issues much more effectively rather than overhauling the whole project when something unexpected happens and renders the project undeployable or unusable.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The flowchart below describes a typical end-to-end workflow of an ML project:\\n\\nHowever, if you’re just starting out in building an end-to-end workflow for your project, this simplified flowchart would be more suitable:\\n\\nThis guide will focus more on the simplified flowchart. If you are looking for what the components represent in the typical end-to-end workflow but not present in the simplified one, you can refer to this section instead.\\n\\n5.8.2 Data Storage\\n\\nThis pertains to storing the lifeblood of all ML/DL projects, of which it is paramount that you know how and where to store data, and that you are able to identify raw and processed data. Raw data is defined as data that is obtained from primary sources such as temperature sensors, while processed data is defined as data that is augmented by algorithm(s) or from another ML/DL model.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It is also important that you know how the product is to be designed, such as how data is being fed into the model, or through a pre-processing algorithm first. You could ingest the data either locally or remotely, and remote options differ between using object storage for blob-like data such as image and audio files, database storage for text data, of which it may be structured (SQL, etc.) or unstructured (NoSQL, JSON, etc.).\\n\\nUnderstanding different types of data storage is important since the data that you may be given may not be suitable to be fed into the model directly. For example, you might be given a compressed folder of Excel sheets, but directly processing them before feeding into the model may be suboptimal when you could parallise both processes and possibly reducing training time. This may also make resuming work easier since there are more checkpoints between the raw data and the model itself.\\n\\nYou can check this section for more information on data storage options.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='5.8. How do I build a basic end-to-end workflow?\\n\\n51\\n\\nAI Practitioner Handbook\\n\\n5.8.3 Version Control\\n\\nMost, if not all, codebases benefit from version control to reduce errors by rolling back changes to a known working source. You may already have been exposed to it through GitHub or GitLab, which are the leading platforms for source code management especially for open source projects. Understanding how to use version control effectively would make the end-to-end workflow easier.\\n\\nFor AI practitioners such as ourselves, there is also a subset of version control that targets toward datasets, which you may read this article for more information.\\n\\n5.8.4 Developer Workspace', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='A workspace is the easiest to understand within the workflow, but can also be quite complex, especially pertaining to authentication and authorisation protocols that your organisation, or client, may have. Some reasons may be due to data being sensitive, the machine being airgapped, or that the code must be compatible with legacy hardware that you cannot interact directly with. Therefore, you may need to check with your project objectives and guidelines to see whether you may need to deploy a remote developer workspace as part of your project infrastructure depending on those protocols.\\n\\n5.8.5 Model Experimentation\\n\\nThis is the main part, of which the workflow is designed around on. A well-designed workflow would make this process run more smoothly since you would not have the need to change the codebase drastically when any one of the other components have to change due to external events beyond your control.\\n\\n5.8.6 Model Serving', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This may well be the end goal for most, if not all, ML/DL projects. Typical tutorials would usually just end it with the model to be served locally through script execution, more mature projects would require a remote callable API since processes are usually interacted between machines rather than only within one machine. These APIs may usually use either the REST or gRPC protocol. You would have to decide which method is more suitable for your project.\\n\\n5.8.7 Final Thoughts\\n\\nThere are many components and processes that can be part of your project’s end-to-end workflow, but if you are starting out on the development process, this guide has recommended the processes you would want to focus on before expanding on other quality-of-life improvements that are described in this section instead.\\n\\n5.8.8 References\\n\\nThe Guide to Data Versioning | LakeFS\\n\\n52\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nAI Practitioner Handbook\\n\\n5.9 How do I enhance my end-to-end workflow?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Contributor(s): Syakyr Surani, Platforms Engineer & Ryzal Kamis, Assistant Head (MLOps)\\n\\nThis guide assumes that you have built a basic end-to-end workflow and want to scale up your workflow while increasing robustness and reliability. If you want to know more about building a basic workflow instead, then you can go to this section instead.\\n\\nAs per from that section, this flowchart describes the typical end-to-end workflow of an AI project:\\n\\nThis section deals with the other components and processes that are present in this workflow, but not the simplified version in the other section.\\n\\n5.9.1 Artifact Storage & Governance\\n\\nArtifacts pertain to data that are not used in training the models in your project. This can be your configuration settings, logs, etc. On the other hand, data and artifact governance refers to the authentication and authorisation protocols that are discussed previously in the Developer Workspace section.\\n\\n5.9.2 CI/CD', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Continuous integration and continuous delivery/deployment processes pertain to the automation of various processes within the workflow itself. While this would benefit end-to-end workflows greatly, it is not implemented within many ML/DL projects due to its complexity and time needed to design an effective CI/CD process. If you do want to know more about CI/CD, you can refer to this section instead.\\n\\n5.9.3 Model Registry\\n\\nThis could be seen as a version control for models to manage problems such as model drifting and retraining. You may need this component as your project scales up, and you need to manage multiple models within the project as well as across multiple projects that may use the same model(s). This component would also make A/B testing easier since you could reference multiple versions of the same model to gauge any improvements a new version may provide.\\n\\n5.9.4 Container Registry', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Containers such as Docker abstracts the codebase while providing a sandbox to reduce dependency conflicts while making component management easier. This allows better scaling and modularity by setting manageable blocks according to their use case instead of by just the products themselves. However, you should take note whether your project requirements allow containerisation as some policies within your organisation/client may be against this process due to the overheads it may have, especially if the project does not require to be designed for scaling.\\n\\n5.9. How do I enhance my end-to-end workflow?\\n\\n53\\n\\nAI Practitioner Handbook\\n\\n5.9.5 Experiment, Pipeline & Model Monitoring/Tracking', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The experiment and pipeline tracking as well as model monitoring can be seen as similar components that target different processes. Experiment and pipeline tracking focuses more towards monitoring the models as they are being trained, and from there, decisions are being made through controlling the hyperparameters of the model, or use a different model architecture altogether.\\n\\nOn the other hand, model monitoring concerns overseeing the trained models and see whether issues are present while it is used for inference such as model drifting. These metrics would be collected and processed to make decisions such as controlling data input requirements or retraining the model.\\n\\n5.9.6 Final Thoughts', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='There are many more components and processes that could be part of your end-to-end workflow that are not discussed in this as well as the basic workflow sections, but the ones that are discussed should be sufficient for you to build a robust and reliable AI end-to-end workflow. If you want to know more about this and MLOps in general, you can take a look at ml-ops.org for more insight.\\n\\n5.9.7 Reference\\n\\nMLOps: Machine Learning Operations | InnoQ\\n\\n5.10 How can I reduce the risks of data poisoning and data extrac-\\n\\ntion?\\n\\nContributor(s): Kew Wai Marn, AI Engineer\\n\\n5.10.1 Data poisoning', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Ideally, data should be collected and labeled in a controlled and safe environment. Practically, this is time-consuming, and thus expensive, which not everyone can afford. Therefore, data is sometimes collected from the Internet or other untrusted sources. This poses a huge risk as an adversary can intentionally manipulate the data, which causes the ML system to be compromised. For example, training data can be injected with specific features that causes the model to fail during inference when inference data with such features are used; i.e. model “backdoors”.\\n\\n5.10.2 Data extraction (inference/inversion attacks)\\n\\nOther than manipulating the data, an adversary may extract details of the training data by querying the model or inspecting it directly. When the adversary has unlimited query access to the model, they can use that to predict whether or not a particular example was contained in the model’s training dataset.\\n\\n54\\n\\nChapter 5. Data Management, Exploration & Processing', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='AI Practitioner Handbook\\n\\n5.10.3 How to check your data sources\\n\\nIn order to reduce the risk of data poisoning and extraction, you should check the following in your ML system:\\n\\n1. Proper access control to protect the raw and processed data from unauthorized users.\\n\\nTo prevent data manipulation, the first thing to do is to secure it (e.g. only authorized personnel are allowed to access within a specified time frame).\\n\\n2. Ensure the integrity of the data sources for your ML system\\n\\nOther than securing your data, it is important to know about the data’s history ie. data generation, collection, and as- sembly process, especially if the source is public. In addition, list down any potential considerations for data quality (e.g. trustworthiness, reliability, etc), so that it is easy to backtrack when necessary.\\n\\nExamples of considerations and measures:\\n\\nHow do you ensure that the raw data was not manipulated or poisoned, even by authorized personnel?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Have you implemented measures in the data collection process to ensure the reliability of the data collected? (e.g.\\n\\nif your raw data is collected from sensors, possible measures to ensure reliability include regular re-calibration,\\n\\nand/or redundant streams with multiple correlated and overlapping sensors)\\n\\nHow do you assure the quality of your labeling process, and validate the resulting labels?\\n\\nThe main idea is to not use data blindly. Take time to understand the data source,. You can also analyse the data to look for inconsistencies i.e. look at feature value range, statistics ie. mean for numerical data, data frequencies for categorical data.\\n\\n3. Ensure that the model output is not part of the input\\n\\nThis is important for models using data crawled from the internet. For example, when translations of pages done by the model were used as training data for the model.\\n\\n4. Model only outputs what the user requires (e.g. no confidence score unless necessary)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This is to prevent an adversary in trying to extract information from your model. For example, for a image classifier, knowing confidence score can allow the adversary to predict whether or not a particular image was contained in the model’s training dataset.\\n\\n5. Limit queries by users\\n\\nAs mentioned earlier, if an adversary were given unlimited query access to a model, they can bombard the model with potential data candidates, in order to predict the distribution of the training data.\\n\\n5.10. How can I reduce the risks of data poisoning and data extraction?\\n\\n55\\n\\nAI Practitioner Handbook\\n\\n6. Anonymize the dataset\\n\\nAs the last line of defense, annoymizing the dataset would prevent any personal identifiable information (PII) from leaking, as per Singapore’s Personal Data Protection Act (PDPA). However, by itself this is not enough as it does not protect the proprietary data.\\n\\nReference(s):\\n\\nBIML Architectural Risk Analysis', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Machine Learning Security against Data Poisoning: Are We There Yet?\\n\\nJust How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks\\n\\nModel Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\\n\\nPDPA Overview\\n\\n56\\n\\nChapter 5. Data Management, Exploration & Processing\\n\\nCHAPTER\\n\\nSIX\\n\\nMODELLING\\n\\n6.1 Overview\\n\\nThis chapter includes sections discussing model training, evaluation and error analysis. Additionally, articles on model explanability and interpretability, and machine learning risks are also included in this chapter.\\n\\n6.2 What are some internal and external considerations when select-\\n\\ning evaluation metrics?\\n\\nContributor(s): Er YuYang, Senior AI Engineer', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This guide assumes that you have a fair understanding of the business value of the AI/ML project, and have an appre- ciation of the implementation difficulty of the evaluation metrics. Internal considerations refer to those concerning the development team, while external considerations refer to those concerning business users/stakeholders.\\n\\n6.2.1 1. Understanding the problem space\\n\\nProblem statement\\n\\nUnderstanding how the AI/ML solves the problem statement helps to identify the correct optimising metrics (explained in section 2) for assessing the performance of the model. Complementarily, understanding the deployment environment helps in choosing the satisficing metrics (explained in section 3). In most cases, optimising metrics tend to be ML metrics, while satisficing metrics tend to be non-ML metrics.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='There is often a trade-off between ML and non-ML metrics. For instance, a more complex model can return better ML metrics, but can also come with longer training and inference times (non-ML metrics). As such, developers and business users are faced with a choice as to which metrics to prioritise based on the business value of the AI solution.\\n\\n57\\n\\nAI Practitioner Handbook\\n\\nTechnical ability of stakeholders\\n\\nIt is important to understand the level of technical ability possessed by eventual end-users and/or maintainers of the AI/ML solution. This understanding can help identify metrics that these stakeholders are comfortable with.\\n\\nComfortable metrics are important for for ease of communicating with the stakeholders. Importantly, they allow future technical teams to understand and improve the AI/ML model in later stages of its life cycle.\\n\\n6.2.2 2. Optimising metrics', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The goal here is to establish a single-number evaluation metric, as multiple evaluation metrics make it hard to compare between different models.\\n\\nFor instance, Classifier A has precision of 95% with recall of 90% and Classifier B has precision of 98% with recall of 85%. Neither classifier is obviously superior and does not immediately guide you towards picking one. In this case, you may consider taking the F1 score, which is the harmonic mean of precision and recall.\\n\\nTo obtain a single-number metric for multi-class classification problems, you can combine metrics of each class into single metric using average or weighted average. However, keep the metrics separated if you are evaluating intraclass performance instead of overall model performance.\\n\\n6.2.3 3. Satisficing metrics', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In theory, optimising metrics should be maximised as much as possible. In practice, however, practical trade- offs/compromises have to be made. These take the form of satisficing metrics, which are often non-ML related. Examples of satisficing metrics include:\\n\\nRunning/inference time\\n\\nTraining time\\n\\nMemory/storage usage (particularly applicable for edge deployment)\\n\\nIt is not practical or cost-efficient to increase optimising metrics by a minuscule percentage while making disproportionate trade-offs on satisficing metrics. For example, increasing model accuracy by 0.0001 may required significantly longer and more expensive computation time. Deployment considerations (eg. size of model, runtime) may also come into play here.\\n\\n6.2.4 4. Combining optimising and satisficing metrics\\n\\nConsideration of both optimising and satisficing metrics means that there are at least two sets of numbers to optimise. How can we adhere more closely to the more elegant single-number metric approach?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='One approach is to derive a single metric through a forumla. For instance:\\n\\nmetrics = accuracy of model - 0.5 * runtime\\n\\nAnother approach is to first define an “acceptable” threshold for the satisficing metric through consultation with stakehold- ers. The development team can then work to maximise the optimising metrics within acceptable bounds of the satisficing metric.\\n\\nOnce your team is aligned on the evaluation metric to optimise, they will be able to make faster progress.\\n\\n58\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\n6.2.5 References\\n\\nMachine Learning Yearning, Chapers 8 and 9\\n\\n6.3 How can I maximise model reproducibility?\\n\\nContributor(s): Kew Wai Marn, AI Engineer\\n\\n6.3.1 Model reproducibilty\\n\\nReproducible models are important because they allow independent practitioners to achieve the same results, which im- proves communication and collaboration. Furthermore, reproducible models are less error-prone and more reliable.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Apart from ensuring reproducibilty in the data and the code of the ML system, you would need to check your model logic as well. Assuming you have the same training code, and the same training data, you should be able to produce the same model.\\n\\n6.3.2 Model reproducibility checklist\\n\\nIf you have the same training code, and the same training data, but you are not able to produce the same model, below are several things you should check to ensure reproducibility of your model. The main goal is to reduce non-deterministic behaviour in ML modelling as much as possible.\\n\\n1. Model tests\\n\\nApart from testing code, you should test your model as well. Below are several things to test your model for.\\n\\na. API calls', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='One of the things you can test is to test the API calls. As ML frameworks and libraries are constantly getting upgraded, you need to make sure the API calls are working as expected. Write unit tests with random input data running through the API call (ie. a single step of gradient descent).\\n\\nTo reduce the risk of API calls breaking, you can version the libraries used; eg. create a software enviroment with specific and static dependency versions recorded in a requirements file.\\n\\nb. Algorithmic correctness\\n\\nOther than testing the API calls, you should make sure your model’s performance is not due to luck.\\n\\nTo check for algorithmic correctness you can:\\n\\nVerify that the loss decreases with increasing training iterations.\\n\\nVerify that without regularization, the training loss is low enough. If your model is complex enough, it will capture\\n\\ninformation from the training data.\\n\\nTest specific subcomputations of your algorithm. (e.g. test that neural network weights are updated with every', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='pass).\\n\\n6.3. How can I maximise model reproducibility?\\n\\n59\\n\\nAI Practitioner Handbook\\n\\nc. Test that model is stable\\n\\nA stable model would be reproducible as it produces consistent results. When training a neural network, your weights and layer outputs should not be NaN or Inf. A given layer should also not return zeroes for more than half of its outputs. Write tests to check for NaN and Inf values in your weights and layer outputs.\\n\\nFurthermore, perform sensitivity analysis on the hyperparameters of your ML system to ensure system hyperparameter stability.\\n\\n2. Deterministically seed the random number generator (RNG)\\n\\nApart from fixing the seed value for data splits, you should use a fixed seed value across all the libraries in the code base in order to reduce non-deterministic behaviours eg. weight initialization, regularization, and optimization in neural networks, split points in random forest algorithm.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Seeding the RNG from Python (i.e. random.seed()) is not enough as some packages have their own implementation of a pseudo-random number generator (e.g. NumPy). Do check their documentation on how they generate the random numbers.\\n\\nHowever, there will still be areas of randomness that are irreducible, such as randomness in third party libraries and in GPUs configurations. Please refer to this section for more information.\\n\\n3. Model version control\\n\\nOther than versioning the data, model versioning is important to ensure reproducibility. By taking snapshots of the entire ML pipeline, it is possible to reproduce the same output again, with the trained weights. Each model version should also be tagged to it’s corresponding metadata (ie. hyperparameters), training data, and code. Model registries and feature stores help in mainining model versions.\\n\\n4. Integration tests', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='As a ML pipeline consiste of several components, tests that runs the entire pipeline end-to-end should be written. To run integration tests more quickly, train on a subset of the data or with a simpler model.\\n\\n5. Logging\\n\\nIt is important to log everything used in ML modelling (e.g. model parameters, hyperparameters, feature transformations, order of features, method to select them, structure of the ensemble (if applicable), hardware specifications). This would help to recreate the environment the model was created in. Furthermore, dependency changes (e.g. changes in API calls) should be recorded.\\n\\n6.3.3 Inherent stochasticity', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='As mentioned earlier, the ML pipeline is not always entirely reproducible. Inherent stochasticity exists in some compo- nents, such as when using GPUs, randomness in backend libraries, and stochastic ML algorithms. In order to improve reproducibility, you can report the average performance of your model using multiple runs or even build an ensemble of models, each trained with a different random number seed. The last resort is to record down any potential stochasticity that exist.\\n\\nReference(s):\\n\\nTesting for Deploying Machine Learning Models\\n\\n60\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\nTesting Pipelines in Production\\n\\nAWS Whitepaper Reproducibility\\n\\nBuilding a Reproducible Machine Learning Pipeline\\n\\nReproducible AI: What it is, Why it Matters & How to Improve it?\\n\\nIntroduction to Random Number Generators for Machine Learning in Python\\n\\nEmbrace Randomness in Machine Learning\\n\\n6.4 How do I assess model robustness?\\n\\nContributor(s): Kew Wai Marn, AI Engineer', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='6.4.1 Overview of Machine Learning Risks\\n\\nRisks of machine learning (ML) systems can come from data, model, and/or infrastructure. According to the architectural risk analysis by the Berryville Institute of Machine Learning, the top risk is adversarial examples. This is when an attacker tries to fool the system by giving malicious input in the form of small perturbations that cause mispredictions. These perturbations are derived from a model’s vulnerabilities, which are areas where the model fails to perform; ie. predict incorrectly.\\n\\nApart from intentional attacks, risks can also come from intrinsic design flaws which can similarly lead to incorrect predictions.\\n\\n6.4.2 What is ML Robustness?', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='According to Singapore’s Model AI Governance Framework, ML robustness is defined as whether the system can function with unexpected inputs. The ability of the model to perform with unexpected inputs affects the reliability of the ML system. Unexpected inputs can be from adversarial or non-adversarial origin.\\n\\nIt is important to consider ML robustness, because non-robust models can fail unexpectedly. For example, consider a road sign detection model deployed in an autonomous vehicle. Misclassify a stop sign as a speed limit sign could result in life-threatening consequences.\\n\\nAdversarial Robustness', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It is important to consider adversarial robustness, because adversarial examples are the top risk to ML systems (as men- tioned earlier). Adverserial robustness is defined in terms of the minimal perturbation value that the attacker must intro- duce for a successful attack (ie. model fails to infer correctly with adversarial input). If a model is adversarially robust, it will require more pertubations for a sucessful attack. It is important to note that there are many other metrics that can be used to measure adversarial robustness (CLEVER, loss sensitivity, etc.).\\n\\n6.4. How do I assess model robustness?\\n\\n61\\n\\nAI Practitioner Handbook\\n\\nNon\\n\\n\\n\\nadversarial Robustness', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='On the other hand, apart from attacks, the ML model itself could be flawed (not able to perform as it should be). Un- expected inputs, mentioned in the definition above, can be quite ambiguous. One type of unexpected inputs can be out-of-distribution (OOD) data. OOD data are data that has a different distribution from your training data but is still within the problem scope where it might appear in production. For example, images from a different camera of the same brand.\\n\\nThis risk can be reduced if data representativeness was considered in the data collection phase, as mentioned in Chapter 1.\\n\\n6.4.3 Robustness Testing\\n\\nOne way to ensure robust ML systems is to do robustness testing, where we can anticipate the robustness of the models using adversarial examples and OOD data.\\n\\nFor adversarial robustness testing, the goal is to test how resilient the model is to adversarial examples, or how easily adversarial examples can be created from the model.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='For non-adversarial robustness testing, the goal is to test where the model fails using OOD data.\\n\\nThere are two ways to obtain OOD data: either collect from another source, or to generate it by applying metamorphic mutation. Metamorphic mutation comes from software testing, where we mutate data that was inferred correctly by the model in a way that does not change the label (label-preserving mutations); eg. horizontally flip an image.\\n\\nFor text, it is slightly trickier as valid mutations depend on your model’s objective. For example, adding typos would be a valid mutation if your model is supposed to be invariant on typos ie. not a spell checker.\\n\\nFor tabular data, mutation strategies are currently still under active research.\\n\\nRobustness Testing Tools\\n\\nHere are some tools you can try for specific types of data, model and task:\\n\\nTool\\n\\nData Type\\n\\nModel Type\\n\\nIBM ART,\\n\\nAdversarial\\n\\nGenerally arrays of numeric data (i.e. Im- ages, Audio), but is attack specific\\n\\nTextAttack,\\n\\nAdversarial', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Text\\n\\nMicrosoft\\n\\nCheck\\n\\n\\n\\nList.\\n\\nadversarial\\n\\nNon\\n\\n\\n\\nAny arbitrary format (as the mutation func- tions can be user-defined). Only some text mutations supported in tool.\\n\\nAny for black box attacks, specific model type for spe- cific white box attacks Any for black box attacks. specific model type for spe- cific white box attacks Any\\n\\nTask Type (Nature\\n\\nof model output)\\n\\nAttack\\n\\n\\n\\nspecific,\\n\\nMainly classification\\n\\nClassification,\\n\\nSequence\\n\\n\\n\\nto\\n\\n\\n\\nsequence\\n\\nAny (as the expecta\\n\\n\\n\\ntion functions can be\\n\\nuser\\n\\n\\n\\ndefined)\\n\\n62\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\nWhen To Perform Robustness Testing?\\n\\nUsing the robustness testing tools, you can test your model in the CI/CD pipeline, at the same level as model evaluation, to get a good evaluation of your model. Just like unit testing or integration testing, you only need to set it up once. With every new retraining, you can compare the robustness between model versions.\\n\\n6.4.4 Words of Caution', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The goal of robustness testing is to find areas where your model does not perform, and improve on them, rather than deciding whether the model is robust or not robust. There is always a need for comparison between models versions (especially for adversarial robustness testing). Testing the robustness of one model does not tell you anything other than where it fails. Lastly, you might find that standard accuracy might have a trade off with robustness, this is still (as of writing) an area under research.\\n\\nReference(s):\\n\\nBIML Architectural Risk Analysis\\n\\nSingapore’s Model AI Governance Framework\\n\\nDeepFool\\n\\nIBM-ART\\n\\nMicrosoft CheckList\\n\\nTextAttack\\n\\nTest ML like software\\n\\nMetamorphic Testing\\n\\n6.5 How do I select classification metrics?\\n\\nContributor(s): Lee Xin Jie, Senior AI Engineer (100E), Er YuYang, Senior AI Engineer (100E)\\n\\n6.5.1 Introduction\\n\\nThe common classification evaluation metrics includes accuracy, confusion matrix, precision, recall, F1, ROC & AUC, precision-recall curve.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This chapter will not elaborate on the definitions of the above mentioned evaluation metrics, as there are numerous resources available for this topic. Instead, this chapter will introduce best practices regarding the selection of classification metrics. This will serve as an extension of the previous chapter What are some internal and external considerations when selecting evaluation metrics, and the concepts discussed here can extend through various domains, be it Natural Language Processing, Computer Vision, etc.\\n\\n6.5. How do I select classification metrics?\\n\\n63\\n\\nAI Practitioner Handbook\\n\\n6.5.2 Imbalanced datasets\\n\\nWhile accuracy is the most common classification metrics, it can be misleading for imbalanced datasets. For example,\\n\\n64\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\nconsider a fraud detection problem where only 5% of the labels are fraudulent. If the model predicts every example to be non fraudulent, it will achieve an accuracy of 95%, which can be very misleading.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In such situations, you may want to consider using precision, recall and F1. Precision measures of all your model’s positive predictions, how many of them are correct. Recall measures of all the actual positive examples, how many of them were identified correctly by the model.\\n\\nPrecision is more important than recall when you cannot afford to have any False Positives as compared to False Negatives. Do take cost into consideration when deciding if precision or recall is more important. When the cost of acting is high and the cost of not acting is low, then precision is preferred. Recall is more important than precision when you cannot afford to have any False Negatives as compared to False Positives. It is more important when the opportunity cost of passing up is high.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The F1 score, also known as F-score, is the harmonic mean of precision and recall. F1 can be computed with the formula. You should choose this metric if you want to balance both precision and recall. 𝐹1 = 2 ⋅ 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ̇𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙 Since there is often an inverse relationship between precision and recall, you might want to weigh one more over the other. This will likely depend on the client’s needs, hence do check with them on their needs before proceeding. You may also want to consider the 𝐹𝛽 score:\\n\\n𝐹𝛽 = (1 + 𝛽2) ⋅\\n\\n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛⋅𝑟𝑒𝑐𝑎𝑙𝑙\\n\\n(𝛽2⋅𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛)+𝑟𝑒𝑐𝑎𝑙𝑙\\n\\nwhere 𝛽 is chosen such that recall is considered 𝛽 times as important as precision.\\n\\nWhy not ROC?\\n\\nThe ROC curve, also known as the receiver operating curve, plots the true positive rate (TPR) vs the false positive rate (FPR) over all classification thresholds from 0 to 1. The AUC, also known as the area under the curve, measures the aggregated performance of your model over all classification thresholds.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The ROC curve, as well as the AUC score, can be misleading in imbalanced datasets. As an example, assume that the positive class is the minority class in an imbalanced dataset. Because the negative class has a lot of examples for the model to learn, the True Negatives are typically very high, especially when compared to the number of false positives. This means that the FPR will likely be low even when there are alot of false positives. Hence this implies that it is possible for a low precision model to achieve a good ROC curve and a high AUC score.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='To illustrate this, assume there are 10 positive examples and 1000 negative examples. There are 9 True Positives, 1 False Negative, 100 False Positives and 900 True Negatives. The TPR is 0.9 while the FPR is 0.1. Since the TPR is high and the FPR is low, the AUC is likely high. This does not mean that the model is good, as the model’s precision is relatively low. In such a scenario, it is preferable to look at the precision recall curve which plots the precision and recall across all thresholds. Likewise, you are also able to compute the AUC of the precision-recall curve, which you can use to compare the performance of various models.\\n\\n6.5. How do I select classification metrics?\\n\\n65\\n\\nAI Practitioner Handbook\\n\\n6.5.3 Multi\\n\\n\\n\\nclass datasets\\n\\nIn multi-class classification, the first evaluation metric you may want to consider is an N × N confusion matrix. An example of which is shown below.\\n\\n66\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Source: Artificial Intelligence Technique for Gene Expression by Tumor RNA-Seq Data: A Novel Optimized Deep Learning Approach\\n\\nWith an N × N confusion matrix, you can quickly identify which classes are performing well, or which classes are commonly misclassified as another class, etc.\\n\\nTo extend precision, recall and F1 to the multi-class domains, we can first compute these scores on a per class basis by using the one-vs-rest approach. This is akin to treating the evaluation of multi-class problems as the evaluation of multiple binary classes.\\n\\nIf you need a single score to compare models, you can compute the averages of precision, recall and F1 using three different averaging techniques.\\n\\n1. Micro: Computes metrics globally by counting the true positives, false positives and false negatives.\\n\\n2. Macro: Computes the unweighted mean of the metrics for all classes, disregarding class imbalance.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='3. Weighted: Computes the metrics for each class, then finds their average weighted by support (the number of true\\n\\ninstances for each label).\\n\\nThe choice of which averaging techniques to use depends on the business objective. If you prefer to treat all classes equally, regardless of their sample size, you can opt for macro-averaging. This could be for situations where the minority class performance is very important to the project sponsor.\\n\\n6.5. How do I select classification metrics?\\n\\n67\\n\\nAI Practitioner Handbook\\n\\nOn the other hand, if it is acceptable to treat each class’s importance based on the class size, you can opt for weighted- or micro-averaging. Do note that for multi-class problems, the micro-averaged F1 score will be the same as the global accuracy.\\n\\n6.5.4 Multi\\n\\n\\n\\nlabel datasets', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='For multi-label problems, you can extend the binary classification metrics by computing each metric per class. This is no different from a binary classification problem. For example, you can compute precision, recall, F1, confusion matrix for each of the classes in a one vs rest approach. Once you have obtained each class’s metrics, you can compute the averages across all classes with the same averaging techniques as covered for multi-class problems: micro, macro, weighted averaging. Note that for multi-label problems, the micro-averaged F1 score is not the same as accuracy.\\n\\n6.5.5 References\\n\\nComprehensive Guide to Multiclass Classification Metrics\\n\\n6.6 What are some common CV evaluation metrics?\\n\\nContributor(s): Calvin Neo, AI Engineer', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This guide assumes a fair understanding of the computer vision-related tasks specifically image classification, object detection, and instance segmentation and their accompanying terms such as bounding boxes (bboxes) and masks. Addi- tionally, it assumes an understanding of classification metrics such as F1 score, precision and recall.\\n\\nThe purpose of this article is to guide AI engineers on key considerations when evaluating metrics for object detection and instance segmentation tasks. For the most part, conventional classification metrics can be used to evaluate image classification tasks. Hence, an understanding of image classification would be useful here. In this case, refer to the article on classification metrics.\\n\\n6.6.1 Mean Average Precision (mAP)\\n\\nThere are many articles on how to calculate mAP available. It is thus instructive to instead discuss considerations when reading mAP.\\n\\nIn discussing mAP evaluation, consider the output below:\\n\\nAverage Precision\\n\\nAverage Precision', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Average Precision\\n\\nAverage Precision\\n\\nAverage Precision\\n\\nAverage Precision\\n\\nAverage Precision\\n\\nMax Detections mAP\\n\\nArea\\n\\nIOU\\n\\n0.743\\n\\n1000\\n\\nall\\n\\n0.5\\n\\n0.213\\n\\n1000\\n\\nall\\n\\n0.75\\n\\n0.326\\n\\n100\\n\\nall\\n\\n0.50:0.95\\n\\n0.127\\n\\n0.50:0.95\\n\\n1000\\n\\nsmall\\n\\n0.349\\n\\n0.50:0.95 medium 1000\\n\\n0.406\\n\\n1000\\n\\nlarge\\n\\n0.50:0.95\\n\\nWhere mAP for IOU = 0.50:0.95 measures the AP values at IOUs 0.50 to 0.95, with a step of 0.05, and then averaging the values.\\n\\nFirstly, it is useful to evaluate the Intersection over Union (IOU) threshold value to understand how “tight” a bbox/mask. A higher IOU threshold value indicates that the model’s inference closely overlaps with the ground truth, and is hence\\n\\n68\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\na more stringent evaluation criteria. Generally, the mAP suffers as the IOU threshold value increases. Thus, evaluating how much these values fall would be valuable clues on how well the model may produce tight bboxes/masks.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Using the outputs above, the second row shows that at IOU threshold of 0.50, the mAP is given by 0.743. Similarly, the third row showing IOU=0.75 threshold shows 0.213, a 0.53 fall in value. This indicates that the bboxes/masks produced by the model were not tight enough, and more training may be required.\\n\\nSecondly, the objects being detected can be broadly separated to small/medium/large objects based on the bbox/mask size. Here, it is assumed the small/medium/large are area of the bbox/mask, and that area threshold for these sizes are clearly defined. For more information on how to determine small/medium/large objects, see article on Exploratory Data Analysis for Compter Vision Tasks.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='A model generally performs the worst on small objects and gets better as the size increases. Thus, evaluating this is also instructive. Considering the above outputs, the mAP is 0.127/0.349/0.406 on small/medium/large objects. Thus, it would be good to have the model be trained on more small objects in order to potentially improve its performance.\\n\\n6.6.2 Frames Per Second (FPS)\\n\\nFPS are generally measured by running multiple images through the pipeline and obtaining the amount of time taken to process each image, before converting it to frames per second. The way to do it is:\\n\\n𝐹 𝑃 𝑆 =\\n\\n1 Average Time Taken Per Image\\n\\nHere, the AI Engineer would need to consider the following:', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='FPS should be calculated based off either model’s or the software architecture’s inference speed. For model’s FPS, it is simply the inference speed solely on the model, without taking into account other processes like data ingestion/pre- processing and inference post-processing. The software architecture’s inference speed considers all the different software components that is related to the inference such as the above mentioned processing steps.\\n\\nIn most cases, though, the FPS of concern would be solely on the model since the main concern is to deliver a model. This is especially so when the concern of FPS is complicated by other factors outside of the model and beyond the AI Engineer’s cotrol, namely internet connections and hardware. Whichever way, the onus is on the AI Engineer to communicate how the FPS was measured to manage expectations.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It is also important to consider what affects FPS. The first factor would be model size. A model that is deep, and hence tend to be larger in size, requires a longer processing time. Another factor is image size. Larger image size will take longer for inference. For example, a 416x416 image will be faster than 1080x960 image. While again this is obvious, it is also often overlooked by AI Engineers when reading papers that purport higher inference speeds, but downplays the role that smaller images have on inference speed.\\n\\n6.6.3 Trade-off Between mAP and FPS', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='When a model has been identified based on mAP and FPS, further refinements can be made that may involve trading FPS for mAP or vice versa. For example, for certain models, the images that is passed in can have its size further reduced. The advantage here would be that inference speed is higher, and hence the FPS will be higher as well. However, since the image is reduced, the model may not have enough features to extract and map, further reducing its ability to accurately infer from the image. This drives down mAP. Increasing the image size will have the opposite effect.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Hence, it is important to establish “good enough”. That is, the engineer would need to work with the sponsor to come up with the metric threshold that would satisfy their use case. For example, if a model currently has mAP = 0.50, and FPS = 60, and the engineer knows that inference speed, for various reasons, would only need to be around 30, then they would be able to increase image size further to potentially increase mAP while accepting the FPS reduction.\\n\\nIt is also good to bear in mind diminishing returns. In the case above, if the FPS went down to 30 and the mAP went up to 0.52, would the trade-off be worth it? No in most cases. Thus, running through various parameters to find a good trade-off is important in such cases.\\n\\n6.6. What are some common CV evaluation metrics?\\n\\n69\\n\\nAI Practitioner Handbook\\n\\n6.6.4 Adjusting Thresholds for Inference - Confidence and Non-max Suppression\\n\\n(NMS)', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='When a model makes an inference, it will give a confidence level of the object’s class. If the confidence does not meet or exceed the threshold, then it will not be included.\\n\\nSimilarly, when a model makes duplicative inference on the same object, meaning it has two or more bounding boxes on the same object, there has to be a mechanism to suppress these duplicative boxes.\\n\\nThus, the engineer may use the Confidence and/or NMS threshold to ensure to adjust the precision and recall of the model.\\n\\nTo show this, consider the output below. Such a table can be generated by running the model on a combination of possible threshold values.\\n\\nConfidence NMS IOU Precision Recall 0.752 0.5 0.3 0.732 0.5 0.5 0.697 0.5 0.7 0.789 0.5 0.3 0.759 0.5 0.5 0.717 0.5 0.7 0.800 0.5 0.3 0.768 0.5 0.5 0.723 0.5 0.7\\n\\n0.697\\n\\n0.750\\n\\n0.811\\n\\n0.640\\n\\n0.718\\n\\n0.794\\n\\n0.579\\n\\n0.682\\n\\n0.774\\n\\n0.1\\n\\n0.1\\n\\n0.1\\n\\n0.4\\n\\n0.4\\n\\n0.4\\n\\n0.7\\n\\n0.7\\n\\n0.7\\n\\nF1 Score\\n\\n0.724\\n\\n0.741\\n\\n0.750\\n\\n0.707\\n\\n0.738\\n\\n0.754\\n\\n0.672\\n\\n0.722\\n\\n0.747', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It is clear here that Confidence and NMS threshold can appear to be at odds with each other. As Confidence Threshold is increases, the Precision increases, but Recall is lower. Conversely, a higher NMS threshold gives better recall.\\n\\n6.6.5 Prioritising Precision or Recall\\n\\nAs stated above, the AI Engineer has to continually engage with the product owner in determining which to prioritise. Obviously, they can also rely on F1-Score to harmonise the two. However, do keep in mind the following:\\n\\n1. Choose Precision if quality is the the priority of the model. That is, it needs to identify objects and their classes.\\n\\n2. Choose Recall if quantity is the priority of the model. That is, it needs to identify many more objects without caring\\n\\ntoo much about whether the class was correctly identified.\\n\\n6.6.6 Conclusion', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This article addresses the conventional metrics for object detection and instance segmentation in the context of pushing a product. Beyond simply calculating these metrics, the AI Engineer would do well look out for granular details when evaluating the metrics in order to give a more accurate picture of model performance.\\n\\n70\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\n6.7 What are some metrics for Named Entity Recognition?\\n\\nContributor(s): Lee Xin Jie, Senior AI Engineer (100E)\\n\\n6.7.1 Named Entity Recognition\\n\\nNamed Entity Recognition refers to the task for locating and classifying named entities in text documents.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This article assumes prior knowledge of the IOB (also known as BIO) format for entity tagging. IOB stands for inside, outside and beginning, where outside (O) indicates a token not belonging to an entity, beginning (B) indicates a token is the start of an entity and inside (I) indicates a token is inside an entity. You may refer to this article for more information on the IOB format.\\n\\nNER can be thought of as a token level classification problem. Hence, NER models are often tuned using metrics such as F1, precision, recall at a token level during model training. When it comes to evaluating NER models on their downstream tasks’ performance, it may be more beneficial to aggregate the individual tokens into full named-entities, and evaluate NER models at a full named-entity level against golden standard annotations. This also makes the evaluation more relatable for the client.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='There are multiple evaluation schemes for evaluating NER models at a full named-entity level. This article will focus on the SemEval (International Workshop on Semantic Evaluation) scheme, which is one of the most popular schemes. The SemEval scheme is built off the Message Understanding Conferene (MUC) scheme.\\n\\nBefore we dive deeper into the SemEval scheme, ley us take a brief look into the MUC scheme. The MUC scheme introduces 5 categories to reflect the correctness of the full named-entities, and the classes are as follows. For a further read up on the MUC scheme, refer to this link\\n\\nEvaluation Scheme\\n\\nCorrect (COR)\\n\\nIncorrect (INC)\\n\\nPartial (PAR)\\n\\nMissing (MIS)\\n\\nSpurius (SPU)\\n\\nSource', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Explanation The output of a system and the golden annotation are the same The output of a system and the golden annotation don’t match System and the golden annotation are somewhat “similar” but not the same A golden annotation is not captured by a system System produces a response which doesn’t exist in the golden annotation\\n\\nIn the SemEval scheme, there are 4 options of measuring the overall performance. They differ in terms of what is considered to be correct, incorrect, partial, missed and spurious predictions. Specifically, these 4 options indicate the correctness of the full named-entities with respect to their boundaries and entity classes. All 4 options can be applied to the computation of F1, precision and recall. A detailed explanation can be found in this blog.\\n\\nEvaluation Scheme\\n\\nStrict\\n\\nExact\\n\\nPartial\\n\\nType\\n\\nSource', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Explanation Exact boundary surface string and entity type Exact boundary match over the surface string, regardless of the type Partial boundary match over the surface string, regardless of the type Some overlap between the system tagged entity and the gold annotation is required\\n\\nTo illustrate the SemEval scheme with an example, consider the same example as before.\\n\\n6.7. What are some metrics for Named Entity Recognition?\\n\\n71\\n\\nAI Practitioner Handbook\\n\\nToken Gold Labels\\n\\nOther\\n\\nThe\\n\\nnews\\n\\nOrg\\n\\nagency Org\\n\\nis\\n\\nhere\\n\\nOther\\n\\nOther\\n\\nNext, we compute the total number of gold annotations with the formula:\\n\\n𝑃 𝑜𝑠𝑠𝑖𝑏𝑙𝑒(𝑃 𝑂𝑆) = 𝐶𝑂𝑅 + 𝐼𝑁 𝐶 + 𝑃 𝐴𝑅 + 𝑀 𝐼𝑆 = 𝑇 𝑃 + 𝐹 𝑁\\n\\nThe total annotations produced by the system is:\\n\\n𝐴𝐶𝑇 𝑈 𝐴𝐿(𝐴𝐶𝑇 ) = 𝐶𝑂𝑅 + 𝐼𝑁 𝐶 + 𝑃 𝐴𝑅 + 𝑆𝑃 𝑈 = 𝑇 𝑃 + 𝐹 𝑃\\n\\nTo compute the precision and recall for Strict and Exact:\\n\\n𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝐶𝑂𝑅\\n\\n𝐴𝐶𝑇 = 𝑇 𝑃\\n\\n𝑇 𝑃 +𝐹 𝑃\\n\\n𝑅𝑒𝑐𝑎𝑙𝑙 = 𝐶𝑂𝑅\\n\\n𝑃 𝑂𝑆 = 𝑇 𝑃\\n\\n𝑇 𝑃 +𝐹 𝑁\\n\\nTo compute the precision and recall for Partial and Type:\\n\\n𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝐶𝑂𝑅+0.5×𝑃 𝐴𝑅\\n\\n𝐴𝐶𝑇', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='𝑅𝑒𝑐𝑎𝑙𝑙 = 𝐶𝑂𝑅+0.5×𝑃 𝐴𝑅\\n\\n𝑃 𝑂𝑆\\n\\n= 𝑇 𝑃\\n\\n𝑇 𝑃 +𝐹 𝑃\\n\\n= 𝐶𝑂𝑅\\n\\n𝐴𝐶𝑇 = 𝑇 𝑃\\n\\n𝑇 𝑃 +𝐹 𝑃\\n\\nTo compute the F1, precision and recall for the example above:\\n\\nMeasure Partial\\n\\nCorrect\\n\\nIncorrect\\n\\nPartial\\n\\nMissed\\n\\nSpurius\\n\\nPrecision\\n\\nRecall\\n\\nF1\\n\\n2\\n\\n0\\n\\n2\\n\\n1\\n\\n1\\n\\n0.6 (a1)\\n\\n0.6 (2)\\n\\n0.6\\n\\nType\\n\\n2\\n\\n2\\n\\n0\\n\\n1\\n\\n1\\n\\n0.4 (b1)\\n\\n0.4 (b2)\\n\\n0.4\\n\\nExact\\n\\n2\\n\\n2\\n\\n0\\n\\n1\\n\\n1\\n\\n0.4 (c1)\\n\\n0.4 (c2)\\n\\n0.4\\n\\nStrict\\n\\n1\\n\\n3\\n\\n0\\n\\n1\\n\\n1\\n\\n0.2 (d1)\\n\\n0.2 (d2)\\n\\n0.2\\n\\nReference for calculation:\\n\\n72\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\na1) 2+0.5×2\\n\\na2) 2+0.5×2\\n\\nb1) 2+0.5×0\\n\\nb2) 2+0.5×0\\n\\n5\\n\\n5\\n\\n5\\n\\n2+0+2+1 = 3\\n\\n2+0+2+1 = 3\\n\\n2+2+0+1 = 2\\n\\n2+2+0+1 = 2\\n\\n2+2+0+1 = 2\\n\\n2+2+0+1 = 2\\n\\n1+3+0+1 = 1\\n\\n1+3+0+1 = 1\\n\\n5\\n\\n5\\n\\n1\\n\\n1\\n\\n5\\n\\n2\\n\\n5\\n\\n5\\n\\n2\\n\\nc1)\\n\\nc2)\\n\\nd1)\\n\\nd2)\\n\\nIn general, partial will be the most lenient evaluation, while strict will be the most stringent evaluation. The scores for type and exact will usually fall in between partial and strict.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='If getting both the boundaries and the classes of the full named-entities are important, then strict will be the measure that you should prioritise. If it is only critical to get the boundaries correct, then exact will be the more appropriate measure to optimise for. Likewise, if it is only critical to get the classes correct and it is acceptable to get partial boundary overlaps, then type will be the more appropriate measure. Lastly, if only partial boundary overlaps are required and it is not important to get the classes correct, then partial can be the preferred measure. Generally, it is more common for type and exact to be preferred. You should check what the client’s requirements are before making a decision as to which to prioritise.\\n\\n6.7.2 References\\n\\nNamed-Entity evaluation metrics based on entity-level\\n\\nInside–outside–beginning (tagging)\\n\\n6.8 How can we evaluate time-series classification models?\\n\\nContributor(s): Andy Ong, AI Engineer', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This guide assumes that you have a fair understanding of typical classification evaluation metrics. This article provides a high-level discussion of the various evaluation metrics used in most projects.\\n\\n6.9 Time\\n\\n\\n\\nseries classification\\n\\nTime-series models can be evaluated using conventional regression and classification evaluation metrics. For regression problems, we can use Mean Squared Error (MSE) or Root Mean Square Error (RMSE). This article focuses on time- series classification problems, where the outcome to be predicted/forecasted is categorical. Recall, precision or F1 can be used to evaluate classification models, depending on business needs.\\n\\nFor a time-series classification project, a simple approach is to view every timepoints individually, and evaluate the model by looking at its F1 Score or precision and recall (similar to a non-time-series classification problem).', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='However, these evaluation metrics might not be adequate to fairly assess time-series models. Firstly, this approach is susceptible to noise. Moreover, it also ignores the autocorrelated nature of time-series data. This autocorrelatio tends to result in sequences of points falling into the same class, rather than one-off occurrences.\\n\\n6.8. How can we evaluate time-series classification models?\\n\\n73\\n\\nAI Practitioner Handbook\\n\\n6.9.1 Time segments\\n\\nConsider the following anomaly detection example:\\n\\nImage credit\\n\\nHere we have 3 ground truth anomalies, and 2 predicted anomalies.\\n\\nIn this approach, we will view timepoints as segments. In the graph below, there are 7 segments, which consist of 2 True Positives (TP), 1 False Positive (FP), 1 False Negatives (FN) and 3 True Negatives (TN). Note that each segment is equally treated as a single instance regardless of its length. This prevents excessive emphasis on long segments.\\n\\nImage credit\\n\\n6.9.2 Overlapping segment', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Another way would be to view any “overlapping” predictions between the ground truth and predictions as 1 continuous True Positive.\\n\\n74\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\nImage credit\\n\\nTaking the above diagram as an example, we can see that there are 2 True Positives. Unlike the earlier approach, no False Positives are recorded here as the relevant ground truth and predicted segments are overlapping. This approach is more lenient, and it rewards the model for any successful detection of anomalies.\\n\\n6.9.3 Case study\\n\\nConsider the following case study to contextualise the two approaches described earlier\\n\\nCompany A is attempting to build an anomaly detection model to predict machinery failures. The ground truth and model prediction values for their model are as follows:\\n\\nT = 0 T = 1 T = 2 T = 3 T = 4 T = 5 T = 6 T = 7\\n\\nGround truth\\n\\nDetected\\n\\n1’s denote anomalies\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='The confusion matrices below are calculated using the time segment and overlapping segment approaches.\\n\\nFrom the table above, we can see that the overlapping approach resulted in a recall of 1.00, which is more lenient as compared to the weighted segment (0.66). As Company A is planning to use the model to detect critical and costly machinery failures, it is not wise to use such a lenient approach like overlapping segment. Instead, the more stringent time segment approach is more appropriate.\\n\\nThis case study demonstrates how business needs must be taken into consideration when deciding the most suitable eval- uation approach is suitable for your time series classification project.\\n\\n6.9. Time\\n\\n\\n\\nseries classification\\n\\n75\\n\\nAI Practitioner Handbook\\n\\n6.10 References\\n\\nTime series anomaly detection—-in the era of deep learning\\n\\n6.11 How can we provide post-hoc explanations for black-box AI\\n\\nmodels?\\n\\nContributor: Joy Lin, Senior AI Technical Consultant\\n\\n6.11.1 Introduction', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Interpretability and explainability have been the recent buzzwords in explainable AI as an attempt to decipher how models derive predictions. As AI is gaining wider acceptance and adoption, stakeholders are increasingly requesting, sometimes under legal requirements, for an explanation on how the model reaches its decision, in order to generate trust in the predic- tions. Scenarios include: providing a valid reason for rejecting a bank loan, or how various features help in categorising a medical disease.\\n\\nExplanations come in useful:\\n\\nduring model development for error analysis and sense-checking\\n\\nafter model deployment to increase stakeholder and end user trust, avoiding significant social or financial impact\\n\\nduring model maintenance to detect drift, bias, or performance changes to guide re-training\\n\\n6.11.2 Interpretable (Glass\\n\\n\\n\\nbox) VS Explainable (Black\\n\\n\\n\\nbox) Models', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='There are two broad approaches to deriving explanations, which is tied to the type of model used: interpretability (using glass-box models) or explainability (using black-box models). The table and diagram below shows the distinction between these approaches.\\n\\nInterpretability (Glass\\n\\n\\n\\nbox)\\n\\nExplainability (Black\\n\\n\\n\\nbox)\\n\\nTransparency Model performance Relationship to model Derivation of explanations Via model’s functional form Outcome Examples\\n\\nHighly translucent\\n\\nLess powerful\\n\\nModel\\n\\n\\n\\nspecific\\n\\nExact explanation\\n\\nLinear regression model coefficients\\n\\nOpaque More powerful Model-agnostic Via post-hoc analysis of model inputs and outputs Approximate explanation Permutation feature importance ranking\\n\\n76\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\n[Source]\\n\\nWith glass-box models, explanations are inherent to the model, and exact.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='With black-box models, explanations require additional post-hoc analysis of the trained model, and explanations are approximate only. Due to this, they need to be used with caution as explanations may vary between runs or when input data changes slightly.\\n\\nIn this article, we focus only on explainability methods for black-box models.\\n\\nA. Understanding feature importance and relationship between inputs and outputs of the model\\n\\nGlobal model-agnostic methods describe how input features affect the output prediction on average. In particular, these methods tell us i) the importance of each feature, and ii) how it impacts the prediction (positive, neutral, or negative). They can be applied to black-box models like tree-based models and neural networks.\\n\\nExamples:\\n\\n1. Permutation feature importance: measures the importance of a feature as an increase in loss when the feature is\\n\\npermuted.\\n\\n6.11. How can we provide post-hoc explanations for black-box AI models?\\n\\n77\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='[Source] In the Titanic dataset, passenger gender is the most important feature to survival outcome, and about twice as important as passenger class and age.\\n\\n1. SHAP (SHapley Additive exPlanations): globally estimates the contribution of every input feature with a combi- nation of high/low and positive/negative Shapley value(s). Useful for sense-checking feature-target relationship.\\n\\n78\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\n[Source] Summary plot of feature importance and feature effects, where each dot represents an instance per feature.\\n\\n1. Partial Dependence Plot (PDP): shows the marginal impact that one or two features have on the predictions. Useful to deep dive and understand the direction of the relationship between specific feature(s) and the target, although limited to easy visualisation of maximum two features.\\n\\n6.11. How can we provide post-hoc explanations for black-box AI models?\\n\\n79\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='[Source] PDP of cancer probability VS the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared to women who had 0 or more than 2 pregnancies. Caution: These relationships should not be interpreted causally.\\n\\nB. Explaining learned representations inside a neural network\\n\\nA neural network can have multiple nodes and layers where input data undergo complex mathematical operations to output predictions. However increasing network density makes it nearly impossible to represent these complexities in a human-readable way. Instead, we use visual representations of data inside a neural network to understand the predictions.\\n\\nExamples:\\n\\n1. Learned features', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='A model can learn - with increasing complexity across the network layers - the various edges, textures, patterns, parts, and eventually objects in images. Assessing these network layers helps you understand the features learned (or missed).\\n\\n80\\n\\nChapter 6. Modelling\\n\\nAI Practitioner Handbook\\n\\n[Source]\\n\\nSimilarly, models can learn features from text or tabular data.\\n\\n2. Pixel attribution using saliency maps\\n\\nSaliency maps provide another visualisation using ranked coloured pixels to indicate their contribution to the model\\n\\n6.11. How can we provide post-hoc explanations for black-box AI models?\\n\\n81\\n\\nAI Practitioner Handbook\\n\\nprediction.\\n\\n[Source] Input image (left) and corresponding saliency map (right).\\n\\n6.11.3 References\\n\\nInterpretable Machine Learning\\n\\nGlass-box models (model-specific)\\n\\nBlack-box models (model-agnostic)\\n\\n82\\n\\nChapter 6. Modelling\\n\\nCHAPTER\\n\\nSEVEN\\n\\n7. SOLUTION DELIVERY\\n\\n7.1 Overview', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This chapter covers matters relating to the deployment of an end-to-end solution, including practices on incremental and continuous deployment.\\n\\n7.2 How can I better understand the client’s deployment require-\\n\\nments?\\n\\nContributor(s): Siti Nuruljannah Baharudin, AI Engineer\\n\\n7.2.1 General\\n\\n1. Where would the final solution be deployed?\\n\\nOptions may include a cloud service, an on-premise server, or an edge device. If the specifications are fixed,\\n\\nthe client should provide them as soon as possible in order for the team to assess whether they are sufficient\\n\\nfor the AI solution to operate on.\\n\\n2. How should the application be packaged for deployment?\\n\\nAlthough containerising the application is a common solution, some organisations may have restrictions on the\\n\\ncontainerisation technology allowed on their servers, or even disallow containerisation altogether. If so, it is\\n\\nrecommended that you assess the impact on the deployment effort. For instance, the team may lack expertise', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='in the containerisation technology used, or when containerisation is not used, more system integration testing\\n\\nmay be required. These would result in additional effort to prepare the deployment package, which may\\n\\nimpact the project schedule.\\n\\n3. Are there any restrictions or preferences regarding the operating system on which the solution should be deployed?\\n\\n4. Are there any restrictions on software licenses to be used?\\n\\nMany open source libraries used in machine learning projects are licensed under Apache 2.0, MIT and BSD\\n\\nlicenses. If your solution requires the use of libraries with less permissive licenses, it would be best to check\\n\\nwith the client.\\n\\n5. Would a staging environment or test device be provided for developers to test with prior to initial deployment?\\n\\nIf not provided, you would need to commission an internal environment that closely mirrors the specifications\\n\\nof the environment on which the solution would eventually be deployed.\\n\\n83\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='7.2.2 Data & Model Outputs\\n\\n1. Where would the data to be used for retraining and inference be located in the deployment environment?\\n\\n2. Where would the model outputs be stored? Is there any storage size limit on this location?\\n\\n3. If the data storage is located remotely, what is the authentication method to be used to access it?\\n\\n7.2.3 Usage\\n\\n1. What is the frequency at which the user intends to run the retraining and inference pipelines?\\n\\nGenerally, the requirements and considerations for inference are more critical compared to retraining, as the\\n\\nlatter is usually performed less frequently and is less time-sensitive.\\n\\n2. Is batch inference a requirement? If so, what is the expected size or volume of data in a single batch?\\n\\n3. What are the requirements on inference speed and training time?\\n\\nYou would need to consider whether the limitations of the chosen deployment device would affect these', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='requirements. If refinements can be made to the code to meet the requirements, the additional effort would\\n\\nalso need to be considered.\\n\\n4. What are the requirements on the size of the trained model?\\n\\n7.2.4 Integration\\n\\n1. Is the application required to interface with another system via an API?\\n\\n2. If an API is to be supported, would the application function as the server or a client?\\n\\n3. Which type of API should be supported?\\n\\nIf the client does not have specific requirements on the API, you may recommend them the most commonly\\n\\nused choice that can be easily supported by the tech stack you are using.\\n\\n4. What is the required format of the API payload?\\n\\nIf the client has provided a specific format, you would need to consider how the size of the payload may affect\\n\\nthe inference speed requirements. Performing some benchmarking tests using sample payload files would\\n\\nenable you to better determine this.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='5. If no API is required, is there any preferred way for users to trigger the training or inference engine?\\n\\n7.3 How can we build a minimum viable configuration for CI/CD au-\\n\\ntomation?\\n\\nContributor(s): Syakyr Surani, Platforms Engineer\\n\\nThis guide assumes that you have some basic knowledge of CI/CD and an existing codebase. The codebase is in a working state and while currently you find it manageable, you anticipate that you need to move away from manual testing that can be unreliable even with a checklist to work with. You may have multiple projects to manage, or there are more contributors raising a pull/merge request than you can possibly handle just by manual testing. You have heard about CI/CD, but you do not know where to start.\\n\\n84\\n\\nChapter 7. 7. Solution Delivery\\n\\nAI Practitioner Handbook', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This guide will not recommend specific CI/CD products or tools, but a general outline to adhere to for an effective minimum implementation, and where to go from there. The flowchart below gives the outline of the guide:\\n\\n7.3.1 Identify Current Processes\\n\\nBefore anything can happen, it is best to plan your actions out. For that, you would need to list out the actions taken to run manual testing for your codebase. This is made easier if you already have a checklist of processes to test before pushing/merging to a branch. Here is a list to check against to see if you have missed anything:\\n\\nLinting\\n\\nError checking (whether the code produces any unexpected errors)\\n\\nPackaging (Docker, PyPI, etc.)\\n\\nSince we focus mainly on AI projects, there should also be a consideration to allow for fast prototyping of model training before packaging the project into a viable product to be interacted by the stakeholders.\\n\\n7.3.2 Create Sample Data', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Due to the nature of our organisation, we deal with data more often than other tech firms. As such, it is imperative that we create and build sample data to test out the functionality of our codebase before running it on the full dataset in order to reduce time taken bugfixing or adding new features.\\n\\n7.3.3 Identify Stages of Deployment\\n\\nThis step is important in order to reduce time taken to test and have less resources used prior to the main events such as training and monitoring during deployment. Some stages of testing can be defined as follows:\\n\\nTests made while committing to a bugfix/feature branch (linting, etc.)\\n\\nTests made before doing a pull/merge request to dev (main) branch (error checking, etc.)\\n\\nTests made from dev branch to release (production) (deployment builds, etc.)\\n\\n7.3.4 Start Small', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This final step is paramount as to not paralyse yourself in the vast overarching process of CI/CD automation. Start with a small subset of the codebase to familiarise yourself with the process before attempting to refactor an entire codebase, especially if you are working in a project with large teams or with critical infrastructure. Iterative development is key to managing a large undertaking, one step at a time.\\n\\nIf you need some guidance on where to start, you could refer to this section of the handbook under the simplified workflow for more information.\\n\\n7.3. How can we build a minimum viable configuration for CI/CD automation?\\n\\n85\\n\\nAI Practitioner Handbook\\n\\n7.3.5 Final Thoughts', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Every project is different, and therefore there is no one Minimum Viable Code/Configuration that can be proposed without first understanding the nature of such projects. Nevertheless, it is with hope that this guide can point out your first steps in building and integrating CI/CD automation into your codebase for a more robust and resilient one.\\n\\nBelow are some of the references you can look at to supplement your CI/CD journey.\\n\\n7.3.6 References\\n\\nGitlab - How to learn CI/CD fast\\n\\nSpiceworks - Top 10 CI/CD Best Practices for 2022\\n\\n86\\n\\nChapter 7. 7. Solution Delivery\\n\\nCHAPTER\\n\\nEIGHT\\n\\nDOCUMENTATION & HANDOVER\\n\\n8.1 Overview\\n\\nThis chapter focuses on the process of knowledge transfer to technical or non-technical teams who are taking over an AI solution.\\n\\n8.2 What are some good practices in documenting system architec-\\n\\nture and processes?\\n\\nContributor(s): Siti Nuruljannah Baharudin, AI Engineer', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In machine learning projects, documentation should be treated like a growing entity that is maintained the way we maintain code. At the same time, we do not want to spend excessive effort documenting details that may rapidly change over the course of the project. In the systems and software engineering fields, modelling languages such as UML are used to document system architecture and processes. Although they will not be discussed here, the concepts from these modelling languages can be adopted to suit the documentation requirements of your project.\\n\\n87\\n\\nAI Practitioner Handbook\\n\\n8.2.1 Use simple visuals\\n\\nBox-and-arrow diagrams are a simple and easily maintainable visual method to document system architecture and pro- cesses. This example shows how a high-level process can be illustrated with a box-and-arrow diagram:\\n\\n8.2.2 Consistency in notation', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='Choose a shape to denote each element and a line to denote each relationship or flow, and stick to these choices for all diagrams used in your documentation. This basic set of notations is a good place to start.\\n\\n88\\n\\nChapter 8. Documentation & Handover\\n\\nAI Practitioner Handbook\\n\\n8.2.3 One diagram per process\\n\\nIllustrate only one process per diagram. diagram for that. I would recommend using the C4 model as a guide. Container, Component and Code - based on a hierarchy of abstractions.\\n\\nIf there is a need to show lower-level detail of a process, create a separate It defines four levels of diagrams - Context,\\n\\n8.2.4 Consider your audience\\n\\nWithin the development team, it may be worth the while to create up to level 3 (Component) diagrams. However, such diagrams may not be useful to non-technical people outside the development team. Furthermore, as they contain imple- mentation details, these diagrams likely get outdated quickly.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='In documentation delivered to end-users, Context diagrams would typically suffice for the general audience, to whom the solution is a black box. On the other hand, if the users are technical people and require understanding the inner workings of the system, including the lower-level diagrams would be a good idea. It is recommended that you automate the generation of Component and Code diagrams to minimise the effort spent on maintaining them.\\n\\n8.2.5 Design considerations\\n\\nColour\\n\\nBesides shapes, colours may also be used to visually differentiate between element types. For instance, in the diagram below, purple is used to denote a remote storage location while yellow is used to denote a local storage location.', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='It is best that you refrain from relying only on colours to relay information in your diagram as there could be readers who have difficulty distinguishing colours. The use of colours should serve to improve understanding of the diagram and increase its visual appeal for the average reader.\\n\\n8.2. What are some good practices in documenting system architecture and processes?\\n\\n89\\n\\nAI Practitioner Handbook\\n\\nLabels\\n\\nAs seen in the earlier examples, every element and relationship in your diagram should be labelled. A label should be specific yet concise, in a legible font.\\n\\nIn a system architecture diagram, the relationships between elements should have the protocol or technology explicitly labelled, as seen in the example below.\\n\\nReference(s):\\n\\nThe C4 model for visualising software architecture\\n\\nBhatti et al. (2021) Docs for Developers: An Engineer’s Field Guide to Technical Writing.\\n\\n90\\n\\nChapter 8. Documentation & Handover\\n\\nCHAPTER\\n\\nNINE\\n\\nCITE AI SINGAPORE’S AI PRACTITIONER HANDBOOK', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       " Document(page_content='This handbook is developed by AI Singapore, with the main purpose of benefiting the in-house engineering staff. However, we hope that it would benefit the public as well and if you would like to cite this content, you may do so with the following snippet for BibTeX:\\n\\n@book{aisg_aiprac_hbook,\\n\\nauthor\\n\\ntitle\\n\\nhowpublished = {\\\\url{https://aisingapore.github.io/ai\\n\\n\\n\\npractitioner\\n\\n\\n\\nhandbook/}},\\n\\nyear\\n\\n= {AI Singapore},\\n\\n= {AI Practitioner Handbook},\\n\\n= {2023}\\n\\n}\\n\\n91', metadata={'source': '../data/pdf_data/AI-Practitioner-Handbook.pdf'})]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total word count: 29592\n",
      "\n",
      "Estimated tokens: 38501\n",
      "\n",
      "Estimated cost of embedding: $0.077002\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# create a GPT-3.5 encoder instance\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "total_word_count = sum(len(doc.page_content.split()) for doc in texts)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in texts)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.002 / 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "# vector_store = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# search_result = vector_store.similarity_search_with_score(\"who is this pdf for?\")\n",
    "# search_result\n",
    "\n",
    "# line_separator = \"\\n\"# {line_separator}Source: {r[0].metadata['source']}{line_separator}Score:{r[1]}{line_separator}\n",
    "# display(Markdown(f\"\"\"\n",
    "# ## Search results:{line_separator}\n",
    "# {line_separator.join([\n",
    "#   f'''\n",
    "#   ### Source:{line_separator}{r[0].metadata['source']}{line_separator}\n",
    "#   #### Score:{line_separator}{r[1]}{line_separator}\n",
    "#   #### Content:{line_separator}{r[0].page_content}{line_separator}\n",
    "#   '''\n",
    "#   for r in search_result\n",
    "# ])}\n",
    "# \"\"\"))\n",
    "\n",
    "# vector_store.save_local(\"C:/Users/meldr/aiap/langchain-question-answer/data/vector_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"../data/vector_data/\"\n",
    "if os.path.exists(path):\n",
    "  vector_store = FAISS.load_local(\n",
    "      path,\n",
    "      OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "  )\n",
    "else:\n",
    "  print(f\"Missing files. Upload index.faiss and index.pkl files to {path} directory first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)  # Modify model_name if you have access to GPT-4\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def print_result(result):\n",
    "    output_text = f\"\"\"### Question: \n",
    "    {query}\n",
    "    ### Answer: \n",
    "    {result['answer']}\n",
    "    ### Sources: \n",
    "    {result['sources']}\n",
    "    ### All relevant sources:\n",
    "    {' '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}\n",
    "    \"\"\"\n",
    "    display(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the different robustness testing tools?\"\n",
    "result = chain(query)\n",
    "print_result(result)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['sources']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/vector_data/\"\n",
    "vector_store = FAISS.load_local(path, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"Use the following pieces of context to answer the users question.\\\n",
    "No matter what the question is, you should always answer it in the context of the AI Practitioner Handbook.\\\n",
    "Even if the question does not end in a question mark, you should still answer it as if it were a question.\\\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\\\n",
    "If the question is not related to the AI Practitioner Handbook, just say that \"I don't know\".\\\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "#question_generator = LLMChain(llm=llm, prompt=prompt)\n",
    "#doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "You can assume the question about the most recent state of the AI Practioner Handbook.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\nYou can assume the question about the most recent state of the AI Practioner Handbook.\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    #question_generator=question_generator,\n",
    "    #combine_docs_chain=doc_chain,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What is the role of an ai engineer?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the role of an ai engineer?',\n",
       " 'chat_history': [],\n",
       " 'answer': ' The role of an AI engineer is to build a solution for another company to take over and implement. They assess whether the client has the capability to take over the final solution, integrate and maintain it, as the goal is to enable companies to build their own AI capabilities in the long run. AI engineers also participate in pre-project scoping and gauge if the client is capable of taking over the AI solution.',\n",
       " 'source_documents': [Document(page_content='2.2 How can I build an effective AI development team?\\n\\nContributor: Kenny WJ Chua, Senior AI Engineer\\n\\n2.2.1 Introduction\\n\\nThe responsbilities of a technical lead are different from those of an individual contributor. Engineers, especially those\\n\\nwho are who are unfamiliar with leadership responsibilities, may face a learning curve in fulfilling these responsibilities.\\n\\nA technical lead indirectly delivers an AI project on time and on target by building an effective AI development team.\\n\\nThis article provides suggestions on promoting effective teams by contextualising general leadership principles.\\n\\nIn AISG, an engineer serves as technical lead for a small team of apprentices (i.e., ‘developers’). Therefore, the contents\\n\\nof this article would be useful to any AI practitioner who is in a similar leadership role.\\n\\nThis article requires some basic knowledge of a AI project life cycle and typical activities that occur in each stage. The', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='ticeship Programme) working on the project full-time for the duration of the project. They are supported by Project\\n\\nManagers and the AI and ML Ops teams to deliver the project on time, and with the right infrastructure and architecture\\n\\npowered by established CI/CD pipelines.\\n\\nOur AI Engineers come from diverse academic disciplines and industry experience. In fact, many of our AI Engineers\\n\\nare hired from AI Apprentice cohorts. This handbook will provide new AI Engineers with a quick guide on our best\\n\\npractices to help them quickly become productive.\\n\\nConclusion\\n\\nSince 2017, we have engaged more than 600 companies, approved more than 1003 projects and completed more than 60\\n\\nof these AI projects under the 100E programme, and trained more than 250 AI Apprentices.\\n\\n2 Who are the AI Apprentices? See How Did AI Singapore Build a “200” Strong All-Singaporean AI Engineering Team With the Blue Ocean\\n\\nStrategy?\\n\\n3 Information to date\\n\\nCONTENTS\\n\\n3\\n\\nAI Practitioner Handbook', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='Contributor: Joy Lin, Senior AI Technical Consultant\\n\\nAI Practitioner Handbook\\n\\n1.5.1 Who is this for?\\n\\nThis article is aimed at AI engineers in an organisation like AI Singapore, who are building a solution for another company\\n\\n(henceforth referred to as the client) to take over and implement. In AI Singapore, the business development/presales\\n\\nteam, together with the AI engineer, scope for feasible AI projects. In doing so, they assess whether the client has the\\n\\ncapability to take over the final solution, integrate and maintain it, as the goal is to enable companies to build their own\\n\\nAI capabilities in the long run. Assuming that the client’s proposed AI solution has an established business value and is\\n\\nethical, this article narrows the focus to the technical aspects of AI readiness.\\n\\n1.5.2 Why does AI engineer need to assess client’s AI capabilities?\\n\\nWhen participating in pre-project scoping, it is beneficial for you to gauge if the client is capable of taking over your', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='We envisioned this handbook as a useful guide for new AI Engineers joining AI Singapore and to quickly come up to speed\\n\\non how we execute AI projects. However, the information contained here would also appeal to any new AI Engineers and\\n\\nManagers deploying their first AI project into production.\\n\\nDelivering production AI models goes beyond building AI models in Jupyter notebooks. It includes preliminary data\\n\\nidentification, cleaning and curation, followed by building, training and testing the models and finally deploying the model.\\n\\nThroughout the process, AI and ML Ops play a fundamental role to enable the whole end-to-end process.\\n\\nThere are many excellent books and resources on various AI algorithms, and we will not replicate them here. Instead, we\\n\\nwill focus on the best practices and knowledge required to deliver an AI project from end to end.\\n\\nCONTENTS\\n\\n1\\n\\nAI Practitioner Handbook\\n\\nIdentifying AI Ready Projects', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history.append((query, result[\"answer\"]))\n",
    "query = \"How can he build the team?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How can he build the team?',\n",
       " 'chat_history': [('What is the role of an ai engineer?',\n",
       "   'According to the context provided, an AI engineer is responsible for building and delivering AI projects on time and on target by building an effective AI development team. They work as technical leads for a small team of apprentices (i.e., ‘developers’), and they are supported by Project Managers and the AI and ML Ops teams to deliver the project on time, and with the right infrastructure and architecture powered by established CI/CD pipelines. Their responsibilities include preliminary data identification, cleaning and curation, followed by building, training and testing the models and finally deploying the model.')],\n",
       " 'answer': 'The article provides suggestions on promoting effective teams by contextualizing general leadership principles. A technical lead indirectly delivers an AI project on time and on target by building an effective AI development team. The article requires some basic knowledge of an AI project life cycle and typical activities that occur in each stage. The technical lead can model the way by demonstrating values and practices that he/she espouses. For example, a technical lead can cultivate a team habit of writing well-structured and documented code by taking on a few of such tasks himself/herself. This may be particularly fruitful when onboarding a new team of junior developers. The article also suggests considering the organizational readiness of the client, such as whether there is an existing technical team who is able to integrate and maintain the AI models, whether the client’s management is supportive of AI projects and team expansion if necessary, and whether the client’s management and key stakeholders allow room for experimentation and development.',\n",
       " 'source_documents': [Document(page_content='2.2 How can I build an effective AI development team?\\n\\nContributor: Kenny WJ Chua, Senior AI Engineer\\n\\n2.2.1 Introduction\\n\\nThe responsbilities of a technical lead are different from those of an individual contributor. Engineers, especially those\\n\\nwho are who are unfamiliar with leadership responsibilities, may face a learning curve in fulfilling these responsibilities.\\n\\nA technical lead indirectly delivers an AI project on time and on target by building an effective AI development team.\\n\\nThis article provides suggestions on promoting effective teams by contextualising general leadership principles.\\n\\nIn AISG, an engineer serves as technical lead for a small team of apprentices (i.e., ‘developers’). Therefore, the contents\\n\\nof this article would be useful to any AI practitioner who is in a similar leadership role.\\n\\nThis article requires some basic knowledge of a AI project life cycle and typical activities that occur in each stage. The', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='2.3 How can I cultivate a cohesive AI development team? . . . . .\\n\\n2.4 What kind of engineering principles can I set for my development team? . . . .\\n\\n2.5 How might we simplify and translate technical jargon? . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. .\\n\\n. . . .\\n\\n.\\n\\n. .\\n\\n.\\n\\n3 Collaborative Development Platforms\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n3.1 Overview .\\n\\n. . .\\n\\n3.2 What are the key platforms required for collaborative ML development? . . . .\\n\\n3.3 What are some considerations in setting up a project repository? . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. .\\n\\n. .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. . . . . . .\\n\\n. . .\\n\\n. . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n4 Literature Review\\n\\n.\\n\\n4.1 Overview .\\n\\n4.2 What are some of the factors to consider during literature review? . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='solution for deployment. Most AI projects end up in failures during deployment phase due to three main reasons:\\n\\n1. Technical hurdles in implementing/integrating model into existing operations\\n\\n2. Decision makers unwilling to approve change to existing operations\\n\\n3. Model performance not considered strong enough by decision makers\\n\\n1.5.3 What can the AI engineer look out for?\\n\\nTo ensure a successful project, let us focus on three broad areas to help you assess the client’s AI readiness level.\\n\\n1. Organisational readiness\\n\\nIs there an existing technical team who is able to integrate and maintain the AI models? If not, what are the\\n\\nclient’s plans to hire the necessary resources?\\n\\nIs the client’s management supportive of AI projects and team expansion if necessary?\\n\\nDo the client’s management and key stakeholders allow room for experimentation and development?\\n\\nAs an organisation looking to use AI solutions, the client’s management should understand that model building and main-', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='article on building a cohesive AI development team may also be useful for engineers in the role of technical lead.\\n\\n2.2.2 Model the way\\n\\nA technical lead may have a specific mental picture of the deliverables and ideal approach for a given task, but the end\\n\\nresult may differ when delegated to developers. One possible reason is imperfect communication, such that developers do\\n\\nnot entirely understand the specifications that have been laid out (hence the need for architecture diagrams). Alternatively,\\n\\nless experienced developers may not understand the need to adhere strictly to certain best practices.\\n\\nIn such a context, it is important for the technical lead to be able to demonstrate some of these values and practices that\\n\\nhe/she espouses. For example, a technical lead can cultivate a team habit of writing well-structured and documented code\\n\\nby taking on a few of such tasks himself/herself. This may be particularly fruitful when onboarding a new team of junior', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history.append((query, result[\"answer\"]))\n",
    "query = \"How does he develop and deloy the solution?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How does he develop and deloy the solution?',\n",
       " 'chat_history': [('What is the role of an ai practitioner?',\n",
       "   \" An AI Practitioner is someone who is responsible for developing and deploying AI projects. This includes tasks such as identifying AI ready projects, building and training models, and deploying models into production. AI Practitioners also need to assess the client's AI capabilities to ensure that they are able to take over the final solution, integrate and maintain it.\"),\n",
       "  ('How can he build the team?',\n",
       "   \" Building an AI team requires recruiting the right people, setting engineering principles, cultivating a cohesive team, and simplifying and translating technical jargon. Additionally, setting up a project repository and assessing the client's AI readiness level are important considerations.\")],\n",
       " 'answer': ' An AI Practitioner develops and deploys an AI solution by first identifying an AI-ready project, then building an effective AI development team, and finally following best practices for model building, training, testing, and deployment.',\n",
       " 'source_documents': [Document(page_content='solution for deployment. Most AI projects end up in failures during deployment phase due to three main reasons:\\n\\n1. Technical hurdles in implementing/integrating model into existing operations\\n\\n2. Decision makers unwilling to approve change to existing operations\\n\\n3. Model performance not considered strong enough by decision makers\\n\\n1.5.3 What can the AI engineer look out for?\\n\\nTo ensure a successful project, let us focus on three broad areas to help you assess the client’s AI readiness level.\\n\\n1. Organisational readiness\\n\\nIs there an existing technical team who is able to integrate and maintain the AI models? If not, what are the\\n\\nclient’s plans to hire the necessary resources?\\n\\nIs the client’s management supportive of AI projects and team expansion if necessary?\\n\\nDo the client’s management and key stakeholders allow room for experimentation and development?\\n\\nAs an organisation looking to use AI solutions, the client’s management should understand that model building and main-', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='We envisioned this handbook as a useful guide for new AI Engineers joining AI Singapore and to quickly come up to speed\\n\\non how we execute AI projects. However, the information contained here would also appeal to any new AI Engineers and\\n\\nManagers deploying their first AI project into production.\\n\\nDelivering production AI models goes beyond building AI models in Jupyter notebooks. It includes preliminary data\\n\\nidentification, cleaning and curation, followed by building, training and testing the models and finally deploying the model.\\n\\nThroughout the process, AI and ML Ops play a fundamental role to enable the whole end-to-end process.\\n\\nThere are many excellent books and resources on various AI algorithms, and we will not replicate them here. Instead, we\\n\\nwill focus on the best practices and knowledge required to deliver an AI project from end to end.\\n\\nCONTENTS\\n\\n1\\n\\nAI Practitioner Handbook\\n\\nIdentifying AI Ready Projects', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='AI Practitioner Handbook\\n\\nContributed by Engineers from AI Singapore\\n\\nEdited by Kenny WJ Chua, Ryzal Kamis,\\n\\nSiavash Sakhavi, Anand Natarajan, Kevin Oh,\\n\\nNajib Ninaba, Kim Hock Ng and Laurence Liew\\n\\nMar 24, 2023\\n\\nCONTENTS\\n\\n.\\n\\n.\\n\\n.\\n\\n1 Pre-project Phase\\n\\n1.1 Overview .\\n\\n.\\n\\n1.2 How can business challenges be translated into AI problems? . . . . .\\n\\n1.3 What are some data considerations when framing an AI project?\\n\\n1.4 What are the considerations for reducing technical debt? . . . . .\\n\\n1.5 How can an engineer assess a client’s AI readiness? . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . .\\n\\n. . . . .\\n\\n. . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . .\\n\\n. .\\n\\n. . . .\\n\\n. . . . . .\\n\\n.\\n\\n. . . .\\n\\n. . . . . .\\n\\n2 Project Management & Technical Leadership\\n\\n.\\n\\n.\\n\\n. . .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n2.1 Overview .\\n\\n.\\n\\n.\\n\\n2.2 How can I build an effective AI development team? . . . . .', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='ticeship Programme) working on the project full-time for the duration of the project. They are supported by Project\\n\\nManagers and the AI and ML Ops teams to deliver the project on time, and with the right infrastructure and architecture\\n\\npowered by established CI/CD pipelines.\\n\\nOur AI Engineers come from diverse academic disciplines and industry experience. In fact, many of our AI Engineers\\n\\nare hired from AI Apprentice cohorts. This handbook will provide new AI Engineers with a quick guide on our best\\n\\npractices to help them quickly become productive.\\n\\nConclusion\\n\\nSince 2017, we have engaged more than 600 companies, approved more than 1003 projects and completed more than 60\\n\\nof these AI projects under the 100E programme, and trained more than 250 AI Apprentices.\\n\\n2 Who are the AI Apprentices? See How Did AI Singapore Build a “200” Strong All-Singaporean AI Engineering Team With the Blue Ocean\\n\\nStrategy?\\n\\n3 Information to date\\n\\nCONTENTS\\n\\n3\\n\\nAI Practitioner Handbook', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'})]}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history.append((query, result[\"answer\"]))\n",
    "query = \"How can he test models?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How can he test models?',\n",
       " 'chat_history': [('What is the role of an ai practitioner?',\n",
       "   \" An AI Practitioner is someone who is responsible for developing and deploying AI projects. This includes tasks such as identifying AI ready projects, building and training models, and deploying models into production. AI Practitioners also need to assess the client's AI capabilities to ensure that they are able to take over the final solution, integrate and maintain it.\"),\n",
       "  ('How can he build the team?',\n",
       "   \" Building an AI team requires recruiting the right people, setting engineering principles, cultivating a cohesive team, and simplifying and translating technical jargon. Additionally, setting up a project repository and assessing the client's AI readiness level are important considerations.\"),\n",
       "  ('How does he develop and deloy the solution?',\n",
       "   ' An AI Practitioner develops and deploys an AI solution by first identifying an AI-ready project, then building an effective AI development team, and finally following best practices for model building, training, testing, and deployment.')],\n",
       " 'answer': ' The best practices for testing models include testing the API calls, checking for algorithmic correctness, testing for model stability, deterministically seeding the random number generator, writing integration tests, and logging.',\n",
       " 'source_documents': [Document(page_content='Apart from testing code, you should test your model as well. Below are several things to test your model for.\\n\\na. API calls\\n\\nOne of the things you can test is to test the API calls. As ML frameworks and libraries are constantly getting upgraded,\\n\\nyou need to make sure the API calls are working as expected. Write unit tests with random input data running through\\n\\nthe API call (ie. a single step of gradient descent).\\n\\nTo reduce the risk of API calls breaking, you can version the libraries used; eg. create a software enviroment with specific\\n\\nand static dependency versions recorded in a requirements file.\\n\\nb. Algorithmic correctness\\n\\nOther than testing the API calls, you should make sure your model’s performance is not due to luck.\\n\\nTo check for algorithmic correctness you can:\\n\\nVerify that the loss decreases with increasing training iterations.\\n\\nVerify that without regularization, the training loss is low enough. If your model is complex enough, it will capture', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='information from the training data.\\n\\nTest specific subcomputations of your algorithm. (e.g. test that neural network weights are updated with every\\n\\npass).\\n\\n6.3. How can I maximise model reproducibility?\\n\\n59\\n\\nAI Practitioner Handbook\\n\\nc. Test that model is stable\\n\\nA stable model would be reproducible as it produces consistent results. When training a neural network, your weights and\\n\\nlayer outputs should not be NaN or Inf. A given layer should also not return zeroes for more than half of its outputs.\\n\\nWrite tests to check for NaN and Inf values in your weights and layer outputs.\\n\\nFurthermore, perform sensitivity analysis on the hyperparameters of your ML system to ensure system hyperparameter\\n\\nstability.\\n\\n2. Deterministically seed the random number generator (RNG)\\n\\nApart from fixing the seed value for data splits, you should use a fixed seed value across all the libraries in the code', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='stores help in mainining model versions.\\n\\n4. Integration tests\\n\\nAs a ML pipeline consiste of several components, tests that runs the entire pipeline end-to-end should be written. To run\\n\\nintegration tests more quickly, train on a subset of the data or with a simpler model.\\n\\n5. Logging\\n\\nIt is important to log everything used in ML modelling (e.g. model parameters, hyperparameters, feature transformations,\\n\\norder of features, method to select them, structure of the ensemble (if applicable), hardware specifications). This would\\n\\nhelp to recreate the environment the model was created in. Furthermore, dependency changes (e.g. changes in API calls)\\n\\nshould be recorded.\\n\\n6.3.3 Inherent stochasticity\\n\\nAs mentioned earlier, the ML pipeline is not always entirely reproducible. Inherent stochasticity exists in some compo-\\n\\nnents, such as when using GPUs, randomness in backend libraries, and stochastic ML algorithms. In order to improve', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'}),\n",
       "  Document(page_content='deciding whether the model is robust or not robust. There is always a need for comparison between models versions\\n\\n(especially for adversarial robustness testing). Testing the robustness of one model does not tell you anything other than\\n\\nwhere it fails. Lastly, you might find that standard accuracy might have a trade off with robustness, this is still (as of\\n\\nwriting) an area under research.\\n\\nReference(s):\\n\\nBIML Architectural Risk Analysis\\n\\nSingapore’s Model AI Governance Framework\\n\\nDeepFool\\n\\nIBM-ART\\n\\nMicrosoft CheckList\\n\\nTextAttack\\n\\nTest ML like software\\n\\nMetamorphic Testing\\n\\n6.5 How do I select classification metrics?\\n\\nContributor(s): Lee Xin Jie, Senior AI Engineer (100E), Er YuYang, Senior AI Engineer (100E)\\n\\n6.5.1 Introduction\\n\\nThe common classification evaluation metrics includes accuracy, confusion matrix, precision, recall, F1, ROC & AUC,\\n\\nprecision-recall curve.', metadata={'source': '../pdf_data/AI-Practitioner-Handbook.pdf'})]}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  [\n",
    "    \"What is the role of  an AI engineer?\",\n",
    "    \"An AI engineer is responsible for developing and implementing artificial intelligence solutions. They work on AI projects, from conceptualization to deployment, and are involved in designing, building, testing, and maintaining AI systems. They also collaborate with other team members, such as data scientists and software developers, to ensure that the AI solutions meet the project requirements and are scalable, efficient, and effective. Additionally, AI engineers may also be responsible for assessing the client's AI capabilities and ensuring that the final solution can be integrated and maintained by the client.\"\n",
    "  ],\n",
    "  [\n",
    "    \"How does he work with the clients?\",\n",
    "    \"According to the AI Practitioner Handbook, an AI engineer should assess a client's AI capabilities by focusing on three broad areas:\\n\\n1. Organizational readiness: The engineer should determine if there is an existing technical team who is able to integrate and maintain the AI models. If not, the engineer should inquire about the client's plans to hire the necessary resources. The engineer should also assess if the client's management is supportive of AI projects and team expansion if necessary, and if they allow room for experimentation and development.\\n\\n2. Technical readiness: The engineer should assess the client's technical infrastructure, including their data storage and processing capabilities, as well as their ability to handle the computational requirements of the AI models.\\n\\n3. Data readiness: The engineer should assess the quality and availability of the client's data, as well as their ability to collect and label data if necessary.\\n\\nBy assessing these areas, the AI engineer can determine if the client is capable of taking over the final solution, integrating and maintaining it, as the goal is to enable companies to build their own AI capabilities in the long run.\"\n",
    "  ],\n",
    "  [\n",
    "    \"Who are the contributors to this book?\",\n",
    "    \"I'm sorry, I couldn't help you with that question. Is there anything else I can assist you with?\"\n",
    "  ],\n",
    "  [\n",
    "    \"Who are the contributors to this book?\",\n",
    "    \"The AI Practitioner Handbook was contributed by Engineers from AI Singapore and edited by Kenny WJ Chua, Ryzal Kamis, Siavash Sakhavi, Anand Natarajan, Kevin Oh, Najib Ninaba, Kim Hock Ng, and Laurence Liew.\"\n",
    "  ],\n",
    "  [\n",
    "    \"How many of them are there? Give me a number.\",\n",
    "    \"There were 7 contributors involved in editing the AI Practitioner Handbook.\"\n",
    "  ]\n",
    "]\n",
    "\n",
    "# AI counted the editors wrongly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questionanswer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
